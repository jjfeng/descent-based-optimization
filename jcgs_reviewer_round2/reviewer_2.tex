\documentclass[]{article}
\usepackage{amsmath}
\usepackage{color}

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\newcommand{\overall}[1]{\textcolor{blue}{#1}}

\newcommand{\point}[1]{\item \textcolor{blue}{#1}}
\newcommand{\reply}{\item[]\ }

\usepackage{cite}
\usepackage{natbib}

\title{Response to Reviewer 2}

\begin{document}
	\maketitle
		
	We appreciate the helpful feedback from the reviewer. We have addressed your questions and comments. Below we give a point-by-point response to each of the questions:
		
	\begin{enumerate}
		\point{In the revised manuscript, the authors have added Table A.1, which includes results for gradient descent, nelder-mead and spearmint for the two parameter case.  However, the results for gradient descent for the "Sparse additive model" are the same as the results reported in Table 2, which were the results with a larger set of regularization parameters.  I think this needs to be corrected.}
		
		\reply We thank the reviewer for finding this mistake. We have corrected the gradient descent results in Table A.1. The gradient descent results are very similar to that of Nelder-mead, Spearmint, and Grid search in the two-parameter case.
		
		\point{In Section 3 of the revised manuscript, the authors added a paragraph discussing the difference between the test and validation set errors in the un-pooled sparse group lasso.  The authors note that "the difference between the validation and test error is greater in gradient descent compared to Nelder-mead and Spearmint."  However, the Spearmint method has a larger difference than gradient descent.  I think this discussion needs to be revised to reflect this aspect of the results.}
		
		\reply We have updated this discussion to accurately reflect the results in Section 3. We have reproduced the updated paragraph below:
		
		\begin{quote}
			In some of the examples, the models with many penalty parameters fit by gradient descent have much smaller validation errors compared to the test errors. ... There are two reasons for this behavior. ... The second reason is that Nelder-Mead is unable to shrink the validation loss as much as gradient descent. In some sense, using Nelder-Mead is similar to ending gradient descent before it has reached convergence. This technique called ``early stopping'' is another form of regularization that can control the degree of over-optimism (Yao et al. 2007).
			\\
			\\
			Spearmint can also give large differences between the validation and test losses, sometimes even larger than gradient descent.
			A possible explanation is that Spearmint performs a more global search over the penalty parameter space when minimizing the validation loss; in contrast, gradient descent tunes penalty parameters in a more local fashion, restricting itself to a more reasonable range and actually descending to a local optimum.
			Thus Spearmint may be considering a larger model space than gradient descent and, consequently, have higher over-optimism.
			Moreover Spearmint struggles to minimize the validation loss since it does not use gradient information to hone-in on a locally optimal set of penalty parameters.
			These two factors may account for its diminished performance compared to gradient descent.
		\end{quote}
		
	\end{enumerate}

\end{document}