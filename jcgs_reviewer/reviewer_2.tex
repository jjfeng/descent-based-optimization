\documentclass[]{article}
\usepackage{amsmath}
\usepackage{color}

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\newcommand{\overall}[1]{\textcolor{blue}{#1}}

\newcommand{\point}[1]{\item \textcolor{blue}{#1}}
\newcommand{\reply}{\item[]\ }

\usepackage{cite}
\usepackage{natbib}

\title{Response to Reviewer 2}

\begin{document}
	\maketitle
		
	We appreciate the helpful feedback from the reviewer. We have addressed your questions and comments. Below we give a point-by-point response to each of the questions:
		
	\begin{enumerate}
		\point{Last paragraph of page 4: you assume that (3) for the training set is strictly convex in $\theta$. I am wondering if the strict convexity assumption would exclude some interesting high-dimensional cases. Can this assumption be removed? Otherwise, it should be listed as a separate condition along with Conditions 1-3 on page 7, as it is a strong condition.}
		
		\reply We thank the reviewer for finding this mistake. The strict convexity assumption is unnecessary, so we have removed it from the paper. Our method only requires the conditions specified in Theorem 1. The conditions are applicable to many popular penalized regression settings, including high-dimensional problems.
		
		\point{Page 7, I found the statement of Definition 3 somewhat disconnected with the rest of the paper. For example, you assume the $n\times p$ matrix $B$ has orthonormal columns. Does this require p is smaller than (or equal to) n? Can you modify this definition directly using the subspace defined earlier?}
		
		\reply We apologize for the misleading notation in Definition 3. $n$ is supposed to be an arbitrary number, not the training size. For clarification, we have replaced $n$ with a different variable $q$. 
		
		
		\point{Examples in Section 2.4. It was said that details are included in the Appendix. I found those details in the Appendix are still very brief. Could you provide more details on how to check Conditions 1-3 for each example in the Appendix? And how about the strong convexity assumption? Is it satisfied, too?}
		
		\reply We have included more details in Section A.3 of the Appendix on how to check Conditions 1-3 for each example. We have also removed the strong convexity assumption. We only require invertibility of the Hessian matrix of the training criterion at its minimizer. This is closely related to the uniqueness of the solution. For instance, the solution to the lasso problem is unique if the covariates are drawn from a continuous probability distribution and therefore satisfies our invertibility condition \citep{tibshirani2013lasso}.
		
		\point{Examples in Section 3. Could you add false positive and false negative in each of the tables? And how about the computational speed? The p in each of the examples is still small. Could you provide some general comments on how well the new algorithm can handle large p?}
		
		\reply We updated Table 6 in Section 4 with false positive and false negative rates. We also streamlined the simulation results in Section 3. We originally included the percent of correctly identified nonzero features, but this metric is misleading. The goal of minimizing the validation error is to minimize the generalization error, not for model recovery or identification of the nonzero features --- in fact in some situations the two are at odds \citep{yang2005can, arlot2010survey}. We have therefore removed these columns.
		
		We compare efficiency of the methods by the number of times the methods needed to solve the inner training criterion. We didn't include computation time since it heavily depends upon the solver used to minimize the training criterion.
		
		We have found that our algorithm scales well with $p$. The primary concern in gradient-based methods that tune the regularization parameter is the computational time when calculating the gradient. We have updated Section 3 to discuss this point in more detail:
		
		\begin{quote}
			 The computational time to calculate the gradient of the validation loss does not grow with the number of model parameters; instead it grows with the dimension of the differentiable/local optimality space. Therefore we can efficiently calculate the gradient of the validation loss as long as the dimension of the differentiable space is small.
		\end{quote}
		
	\end{enumerate}

\bibliographystyle{agsm}
\bibliography{reviewer_2}

\end{document}