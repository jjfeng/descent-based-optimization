\documentclass[]{article}
\usepackage{amsmath}
\usepackage{color}

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\newcommand{\overall}[1]{\textcolor{blue}{#1}}

\newcommand{\point}[1]{\item \textcolor{blue}{#1}}
\newcommand{\reply}{\item[]\ }

%opening
\title{Response to Reviewer 1}

\begin{document}
	
	\maketitle
	
	We appreciate the helpful feedback from the reviewer. We have addressed your questions and comments. Below we give a point-by-point response to each of the questions posed by the reviewer:
	
	\subsubsection*{Overall comment}
	\overall{Although the idea of a using a larger set of regularization parameters is interesting, the empirical results are incomplete and including additional scenarios would help strengthen the paper}
		
	We have included more details in the empirical results and included a new example (I don't know what though).	
		
	\subsubsection*{Specific Suggestions/comments}
	
	\begin{enumerate}
		\point{Can the authors point to examples in the literature where a large set of regularization parameters was used?}
		
		\reply We have added more examples to Section 1.
		
		\point{First line on p. 13, ``the optimal regularization parameters $\boldsymbol{\lambda} = (\lambda_1,\lambda_2)^\top$'' should be ``the optimal regularization parameters $\boldsymbol{\lambda} = (\lambda_0, \lambda_1, ... , \lambda_M)^\top$''}
		
		\reply We have corrected this typo.
		
		\point{The authors note that the models considered in Sections 2.4.2 and 2.4.3 are usually employed with only two regularization parameters but propose using a larger set of regularization parameters. In the corresponding simulations in Sections 3.2 and 3.3, they compare using a larger set of regularization parameters against using two regularization parameters selected using grid search. The grid spaces considered are fairly small, so it’s not clear if the improved performance for gradient descent is due to the additional regularization parameters or due to the dependence of performance on the grid space. To provide additional insight into what’s causing the difference in performance, could the authors also present results using gradient descent, Nelder-mead, and Spearmint for the two parameter case?}
		
		\reply We have added the results from gradient descent, Nelder-mead, and spearmint for the two-parameter case for all the examples from Section 3 in the appendix.
		
		\point{The authors report average performance and standard errors for the simulations done in Section 3. How many simulation runs were used in each example?}
		
		\reply We have added a sentence regarding the number of simulation runs in the beginning of Section 3.
		
		\point{In Section 3, two different starting values were considered for Nelder-mead and gradient descent. How sensitive were the results to the choice of starting values?}
		
		\reply We tried increasing the number of initializations for Nelder-mead and gradient descent in the un-pooled sparse group lasso and present the results in the Appendix. Both methods are indeed sensitive to their initializations, but gradient descent can find penalty parameters with lower validation error than Nelder-mead.
		
		\point{The empirical results for gradient descent depend on $\alpha$, $\beta$, and $\delta$. The authors mention ranges considered for these parameters in Section 2.5. How were these parameters ultimately selected in the evaluations in Sections 3 and 4?}
		
		\reply Since the results were not sensitive to the choice of $\alpha$,$\beta$, and $\delta$, we have set the values to be the same in all simulations. We have specified their values in Section 2.5.
		
		\point{In Sections 3.2 and 3.3, the authors created a training, validation, and test set, but in Section 3.1 they only consider a training and validation set. Why was a test set not considered in Section 3.1?}
		
		\reply The goal of Section 3.1 was just to illustrate that gradient descent was able to find a validation loss that was nearly the same as grid search. To streamline the paper and to make the empirical results more compelling, we have included a test set to Section 3.1.
		
		\point{Table 1 should note that ”standard errors are given in parentheses.”}
		
		\reply This has been corrected.
		
		\point{For the simulations in Section 3.3, why did the authors set n = 60 in the first case and
n = 90 in the other two cases?}
		
		\reply We have changed the training size for all simulations in Section 3.3 to $n=90$ for consistency.
		
		\point{$g$ is undefined in Table 4}
		
		\reply We had meant for $g$ to denote the true number of groups, but this was already given in equation (??). We have removed it.
		
		\point{In Table 4, could the authors provide intuition for why there is a large difference in validation error and test error for gradient descent?}
		
		\reply We have added a paragraph in Section 3.3 (??check this again later) to provide some intuition for the observed behavior.
	\end{enumerate} 
	
\end{document}
