\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{color}


% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

%%%% Packages and definitions
\usepackage{xr}
\externaldocument{hillclimbing_nonsmooth}

\usepackage[top=0.85in,left=1.0in,right=1.0in,footskip=0.75in]{geometry}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

\usepackage{algorithm,algorithmic}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

\usepackage{amsthm}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% for the beautiful checkmarks
\usepackage{pifont}

\DeclareMathOperator*{\argmin}{arg\,min}
%%%%

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\thealgorithm}{}

\section{Appendix}

\subsection{$K$-fold Cross Validation}
We can perform joint optimization for $K$-fold cross validation by reformulating the problem. Let $(\boldsymbol y, \boldsymbol{X})$ be the full data set. We denote the $k$th fold as $(\boldsymbol y_{k}, \boldsymbol{X}_{k})$ and its complement as $(\boldsymbol y_{-k}, \boldsymbol{X}_{-k})$. Then the objective of this joint optimization problem is the average validation cost across all $K$ folds:
\begin{equation}
\begin{array}{c}
\argmin_{\boldsymbol{\lambda} \in \Lambda} \frac{1}{K} \sum_{k=1}^K L(\boldsymbol{y}_{k}, f_{\hat{\boldsymbol \theta}^{(k)}(\boldsymbol{\lambda})}(\boldsymbol{X}_k)) \\
\text{s.t. } {\hat{\boldsymbol \theta}^{(k)}(\boldsymbol{\lambda})} = \argmin_{\boldsymbol \theta \in \Theta} L(\boldsymbol{y}_{-k}, f_{\boldsymbol \theta} (\boldsymbol{X}_{-k})) + \sum\limits_{i=1}^J \lambda_i P_i(\boldsymbol \theta) \text{ for } k=1,...,K
\end{array}
\label{jointoptFullCV}
\end{equation}

\subsection{Proof of Theorem~\ref{thethrm}}

\begin{proof}
We will show that for a given $\boldsymbol \lambda_0$ that satisfies the given conditions, the validation loss is continuously differentiable within some neighborhood of $\boldsymbol \lambda_0$.  It then follows that if the theorem conditions hold true for almost every $\boldsymbol \lambda$, then the validation loss is continuously differentiable with respect to $\boldsymbol \lambda$ at almost every $\boldsymbol \lambda$.

Suppose the theorem conditions are satisfied at $\boldsymbol \lambda_0$. Let $\boldsymbol B'$ be an orthonormal set of basis vectors that span the differentiable space $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$ with the subset of vectors $\boldsymbol B$ that span the model parameter space.

Let $\tilde L_T(\boldsymbol \theta,\boldsymbol \lambda)$ be the gradient of $L_T(\cdot, \boldsymbol \lambda)$ at $\boldsymbol \theta$ with respect to the basis $\boldsymbol B$:
\begin{equation}
\tilde L_T(\boldsymbol \theta,\boldsymbol \lambda) = _{\boldsymbol B}\nabla L_T(\cdot, \boldsymbol \lambda) |_{\boldsymbol \theta}
\end{equation}

Since $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$ is the minimizer of the training loss, the gradient of $L_T(\cdot, \boldsymbol \lambda_0)$ with respect to the basis $\boldsymbol B$ must be zero at $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$:
\begin{equation}
_{\boldsymbol B}\nabla L_T(\cdot, \boldsymbol \lambda_0)|_{\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)} = \tilde L_T(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0) = 0
\end{equation}

From our assumptions, we know that there exists a neighborhood $W$ containing $\boldsymbol \lambda_0$ such that $\tilde L_T$ is continuously differentiable along directions in the differentiable space $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$. Also, the Jacobian matrix $D \tilde L_T(\cdot, \boldsymbol \lambda_0)|_{\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)}$ with respect to basis $\boldsymbol B$ is nonsingular. Therefore, by the implicit function theorem, there exist open sets $U \subseteq W$ containing $\boldsymbol \lambda_0$ and $V$ containing $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$ and a continuously differentiable function $\gamma: U \rightarrow V$ such that for every $\boldsymbol \lambda \in U$, we have that 
\begin{equation}
\tilde L_T(\gamma(\boldsymbol \lambda), \boldsymbol \lambda) = \nabla_{B} L_T(\cdot, \boldsymbol \lambda)|_{\gamma(\boldsymbol \lambda)} = 0
\end{equation}
That is, we know that $\gamma(\boldsymbol \lambda)$ is a continuously differentiable function that minimizes $L_T(\cdot, \boldsymbol \lambda)$ in the differentiable space  $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$.
Since we assumed that the differentiable space is a local optimality space of $L_T(\cdot, \boldsymbol \lambda)$ in the neighborhood $W$, then for every $\boldsymbol \lambda \in U$, 
\begin{equation}
\hat {\boldsymbol \theta}(\boldsymbol \lambda) =
\argmin_{\boldsymbol \theta} L_T(\boldsymbol \theta, \boldsymbol \lambda) =
\argmin_{\boldsymbol \theta \in \Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)} L_T(\boldsymbol \theta, \boldsymbol \lambda) =
\gamma(\boldsymbol \lambda)
\end{equation}
Therefore, we have shown that if $\boldsymbol \lambda_0$ satisfies the assumptions given in the theorem, the fitted model parameters $\hat {\boldsymbol \theta}(\boldsymbol \lambda)$ is a continuously differentiable function within a neighborhood of $\boldsymbol \lambda_0$. We can then apply the chain rule to get the gradient of the validation loss.
\end{proof}

\subsection{Regression Examples}

\textcolor{red}{Did we give more details on showing that our conditions are met? (as asked for by the reviewer?). We don't need to give a ton more details, but we should do \emph{something}}

\subsubsection{Elastic Net}\label{enet_conditions}
We show that the joint optimization problem for the Elastic Net satisfies all three conditions in Theorem~\ref{thethrm}:
\begin{itemize}
\item[] Condition 1: The elastic net solution paths are piecewise linear \citep{zou2003regression, tibshirani2013lasso}, which means that the nonzero indices of the elastic net estimates stay locally constant for almost every $\boldsymbol{\lambda}$. Therefore, $S_{\boldsymbol{\lambda}}$ is a local optimality space for $L_T(\cdot, \boldsymbol{\lambda})$. \hfill \ding{51}
\item[] Condition 2: The $\ell_1$ penalty is smooth when restricted to $S_{\boldsymbol{\lambda}}$.\hfill \ding{51}
\item[] Condition 3: The Hessian matrix of $L_T(\cdot, \boldsymbol{\lambda})$ with respect to the columns of $\boldsymbol I_{I(\boldsymbol \lambda)}$ is $\boldsymbol I_{I(\boldsymbol \lambda)}^\top \boldsymbol{X}_{T}^\top \boldsymbol{X}_{T} \boldsymbol I_{I(\boldsymbol \lambda)} + \lambda_2 \boldsymbol{I}$. This is positive definite if $\lambda_2 > 0$. \hfill \ding{51}
\end{itemize}

\subsubsection{Additive Models with Sparsity and Smoothness Penalties}
\label{sec_appendix:sparse_add_models}
Let 
\begin{equation}
\boldsymbol{U} = \begin{bmatrix}
\boldsymbol {U}^{(i_1)} & ... & \boldsymbol {U}^{(i_{|J(\boldsymbol \lambda)|})}
\end{bmatrix}
\end{equation}
where $i_\ell \in J(\boldsymbol \lambda)$. Then
\begin{equation}
\boldsymbol{H}(\boldsymbol\lambda)
= \boldsymbol{U}^\top \boldsymbol I_T^\top \boldsymbol I_T \boldsymbol{U}
+ \lambda_0 \text{ } diag \left (
\frac{1}{||\boldsymbol {U}^{(i)}  \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)||_2} \left (
\boldsymbol I - \frac{\hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)^\top \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)}{||\boldsymbol {U}^{(i)}  \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)||_2}
\right ) \right )
+ \epsilon \boldsymbol I
\label{eq:add_hessian}
\end{equation}
Now we check that all three conditions are satisfied.
\begin{itemize}
	\item[] Condition 1: It seems likely that the space spanned by the nonzero coefficients of $\boldsymbol{\theta}$ is a local optimality space, though we are unable to formally prove this fact. Empirically, it has been found that the group lasso solution paths are smooth almost everywhere \citep{yuan2006model}. On the theoretical side, \citet{vaiter2012degrees} proved that the active set in a group lasso problem is locally constant for small perturbations in the response. Similar techniques can probably be used to show that the active set is locally constant for small perturbations in the penalty parameters.
	% However, the dual formulation of this problem suggests that this condition holds. In particular, if we create dummy constraints $\boldsymbol{z}^{(i)} = \boldsymbol{I}_T\boldsymbol{\theta}^{(i)}$ and $\boldsymbol{w}^{(i)} = \boldsymbol{D}_{x_i}^{(2)} \boldsymbol{\theta}^{(i)}$ with corresponding dual variables $\boldsymbol{u}^{(i)}$ and $\boldsymbol{v}^{(i)}$, we find that the dual variables must satisfy constraints of the form $\|\boldsymbol{I}_T^\top \boldsymbol{u}^{(i)}\|_2 \le \lambda_0$ and $\|\boldsymbol{v}^{(i)}\|_\infty \le \lambda_i$. It therefore seems likely that the dual solution perturbs smoothly with $\boldsymbol{\lambda}$. Under some regularity conditions, there is also a smooth mapping between the primal and dual solutions. Hence the fitted model probably smooth with respect to $\boldsymbol{\lambda}$. Finally, assuming that the space spanned by the nonzero coefficients of $\hat{\boldsymbol{\theta}}\boldsymbol{\lambda}$ is a local optimality space, then clearly the local optimality space is also a differentiable space.
	\hfill \ding{51}\textbf{?}
	\item[] Condition 2: The $\ell_1$ and $\ell_2$ penalties are smooth when restricted to $S_{\boldsymbol{\lambda}}$.\hfill \ding{51}
	\item[] Condition 3: The Hessian matrix in \eqref{eq:add_hessian} is positive definite for any $\epsilon > 0$. \hfill \ding{51}
\end{itemize}

The matrix $C(\boldsymbol \beta( \boldsymbol \lambda))$ in \eqref{eq:additive_gradient} is defined as
\begin{equation}
C(\boldsymbol \beta( \boldsymbol \lambda))
= \begin{cases}
\begin{bmatrix}
\boldsymbol{0} \\
\boldsymbol {U}^{(i)\top}  \boldsymbol{D}^{(2)\top}_{\boldsymbol{x}_i} 
sgn( \boldsymbol{D}^{(2)}_{\boldsymbol{x}_i} \boldsymbol {U}^{(i)} \hat{\boldsymbol{\beta}}^{(i)}) \\
\boldsymbol{0} \\
\end{bmatrix}
& \text{ for } i \in J(\boldsymbol \lambda) \\
\boldsymbol{0}
& \text{ for } i \not\in J(\boldsymbol \lambda) \\
\end{cases}
\end{equation}


\subsubsection{Un-pooled Sparse Group Lasso}
The Hessian in this problem is
\begin{equation}
\boldsymbol{H}(\boldsymbol\lambda) =
\frac{1}{n} \boldsymbol X_{T, I(\boldsymbol \lambda)}^\top \boldsymbol X_{T, I(\boldsymbol \lambda)}
+ diag\left(
\frac{\lambda_m}{|| \boldsymbol \theta^{(m)}||_2}
\left (
\boldsymbol I - 
\frac{\boldsymbol \theta^{(m)} \boldsymbol \theta^{(m) \top}}{|| \boldsymbol \theta^{(m)}||_2^2}
\right )
\right)
+ \epsilon \boldsymbol I
\label{eq:sgl_hessian}
\end{equation}
The logic for checking all three conditions in Theorem~\ref{thethrm} is similar to the other examples:
\begin{itemize}
	\item[] Condition 1: The space spanned by the nonzero coefficients of $\boldsymbol{\theta}$ is clearly also a differentiable space. We hypothesize that this space is also a local optimality space $\boldsymbol{\lambda}$, though we have not formally proven this fact. We suspect this to be true for the same reasons discussed in Section \ref{sec_appendix:sparse_add_models}.
	%it is difficult to formally prove that the local optimality space at $\boldsymbol{\lambda}$ is the space spanned by the nonzero coefficients of $\boldsymbol{\theta}$. However if we follow very similar reasoning, the dual formulation of this problem suggests that the fitted model perturbs smoothly in $\boldsymbol{\lambda}$ almost everywhere. Hence it seems likely that the set of nonzero coefficients $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$ is constant at almost every $\boldsymbol{\lambda}$. Assuming this is true, then the local optimality space $S_{\boldsymbol{\lambda}}$ is clearly also a differentiable space.
	\item[] Condition 2: The $\ell_1$ and $\ell_2$ penalties are twice-differentiable when restricted to $S_{\boldsymbol{\lambda}}$.\hfill \ding{51}
	\item[] Condition 3: The Hessian matrix in \eqref{eq:sgl_hessian} is positive definite for any $\epsilon > 0$. \textcolor{red}{why...?} \hfill \ding{51}
\end{itemize}


The matrix $\boldsymbol C(\hat {\boldsymbol \beta}(\boldsymbol \lambda))$ in \eqref{eq:unpooled_sgl_grad} has columns $m=1,2...,M$ 
\begin{equation}
\begin{bmatrix}
\boldsymbol 0\\
\frac{\hat {\boldsymbol \beta}^{(m)}(\boldsymbol \lambda)}{||\hat{\boldsymbol \beta}^{(m)}(\boldsymbol \lambda)||_2}\\
\boldsymbol 0\\
\end{bmatrix}
\end{equation}
where $\boldsymbol 0$ are the appropriate dimensions.

\subsubsection{Low-rank Matrix Completion}
We first show that a valid differentiable space of the training criterion \eqref{eq:matrix_comp_groups} is where the rank of the interaction matrix $\Gamma$ stays constant \textcolor{red}{(more precise.)}. Suppose the fitted interaction matrix $\Gamma$ has SVD decomposition $U\Sigma V^\top$ where the $i$-th singular value is denoted $\sigma_i$. The subdifferential of the nuclear norm is \textcolor{red}{Cite}
\begin{equation}
\partial \| \Gamma \|_* = 
\left \{
U diag(\mu) V^\top \middle | 
\mu_i \in 
\begin{cases}
[-1, 1] & \sigma_i = 0\\
sign(\sigma_i) & \sigma_i \ne 0\\
\end{cases}
\right \}
\end{equation}
The subdifferential reduces to a gradient if we differentiate with respect to  matrices of rank no larger than the rank of $\Gamma$.

To derive the gradient of the validation error with respect to the penalty parameters, we perform implicit differentiation of the gradient optimality conditions to get a system of linear equations. However one must be careful in determining the gradient conditions. In particular, we need to transform the subgradient optimality conditions to get gradient optimality conditions. 
Let the SVD decomposition of $\hat{\boldsymbol{\Xi}}(\boldsymbol{\lambda})$ be $\hat{\boldsymbol{U}}(\boldsymbol{\lambda}) \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})  \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top$.
After taking the subgradient of the training criterion with respect to $\boldsymbol{\Xi}$, we must multiply the result by its left singular vectors to get the following gradient optimality condition:
\begin{align}
\boldsymbol{0} & = 
- \frac{1}{|T|} 
\hat{\boldsymbol{U}}(\boldsymbol{\lambda})^\top
\left (
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  \boldsymbol{1}^\top )^\top
- \hat{\boldsymbol{\Xi}}
\right )\\
& \qquad + \lambda_0 sign(\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
+ \epsilon \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda}) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
\end{align}
Similarly, we multiply the result by its right singular vectors to get
\begin{align}
\boldsymbol{0} & = - \frac{1}{|T|} 
\left (
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  \boldsymbol{1}^\top )^\top
- \hat{\boldsymbol{\Xi}}
\right )
\hat{\boldsymbol{V}}(\boldsymbol{\lambda})\\
& \qquad + \lambda_0 \hat{\boldsymbol{U}}(\boldsymbol{\lambda}) sign(\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})) 
+ \epsilon \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda}) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})
\label{eq:grad_opt_matrix_comp}
\end{align}
To get additional linear constraints, we implicitly differentiate the conditions $\boldsymbol{\hat U}^\top(\boldsymbol{\lambda}) \boldsymbol{\hat{U}}(\boldsymbol{\lambda}) = \boldsymbol{I}$ and $\boldsymbol{\hat{V}}(\boldsymbol{\lambda})^\top \boldsymbol{\hat{V}}(\boldsymbol{\lambda}) = \boldsymbol{I}$ with respect to $\boldsymbol{\lambda}$. Combining this with the gradient optimality conditions with respect to $\eta$ and $\gamma$, we can derive the gradient of the validation loss with respect to $\boldsymbol{\lambda}$. Note that the solution to this system of linear equations will give us the partial derivatives $\frac{\partial}{\partial \boldsymbol{\lambda}}\hat{\boldsymbol{U}}(\boldsymbol{\lambda})$, $\frac{\partial}{\partial \boldsymbol{\lambda}}\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})$, and $\frac{\partial}{\partial \boldsymbol{\lambda}}\hat{\boldsymbol{V}}(\boldsymbol{\lambda})$. From this, we can derive $\frac{\partial}{\partial \boldsymbol{\lambda}}\hat{\boldsymbol{\Xi}}(\boldsymbol{\lambda})$.

Finally, we check the three conditions in Theorem~\ref{thethrm}. The reasoning is is similar to the other examples:
\begin{itemize}
	\item[] Condition 1: In Section \ref{sec:matrix_completion}, we showed that the differentiable space of the joint optimization problem at $\boldsymbol{\lambda}$ is the space of matrices are the same rank as $\hat{\boldsymbol{\Gamma}}(\hat{\boldsymbol{\lambda}})$ and the space spanned by the nonzero row and column groups of $\hat{\boldsymbol{\alpha}}(\boldsymbol{\lambda})$ and $\hat{\boldsymbol{\beta}}(\boldsymbol{\lambda})$. We hypothesize that this is also a local optimality space $\boldsymbol{\lambda}$. This seems to be justified for the same reasons mentioned in \ref{sec_appendix:sparse_add_models}. In addition, it has been shown empirically that for matrix completion problems with a nuclear norm penalty, similar penalty parameter generally result in fitted matrices of identical rank \citep{mazumder2010spectral}.
	%is the space spanned by the nonzero coefficients of $\boldsymbol{\theta}$. However if we follow very similar reasoning, the dual formulation of this problem suggests that the fitted model perturbs smoothly in $\boldsymbol{\lambda}$ almost everywhere. (Note that in the dual formulation, the constraints are now with respect to the spectral norm.) Hence it seems likely that the rank of the matrix stays constant at almost every $\boldsymbol{\lambda}$. Assuming this is true, then the local optimality space $S_{\boldsymbol{\lambda}}$ is clearly also a differentiable space.
	\item[] Condition 2: The nuclear norm penalty and the group lasso penalties are twice-differentiable when restricted to $S_{\boldsymbol{\lambda}}$. \textcolor{red}{mention that you do this calculation above for NN, and in text for others} \hfill \ding{51}
	\item[] Condition 3: The Hessian of the training criterion is positive definite for any $\epsilon > 0$ since the nuclear norm and the group lasso penalties are convex and the frobenius norm and the ridge penalties are strongly convex. \textcolor{red}{Do we have an issue here because we don't have a frobenius norm on all entries?} \hfill \ding{51}
\end{itemize}



%\begin{align}
%\boldsymbol{0} & = 
%\frac{1}{|T|} 
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{U}}(\boldsymbol{\lambda})^\top
%\left (
%\boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) 
%\boldsymbol{1}^\top 
%+ \left (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  
%\boldsymbol{1}^\top \right )^\top
%+ \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\Xi}}
%\right ) \\
%& \qquad \lambda_0 sign(\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
%+ \epsilon \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda}) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top\\
%% next equation
%\boldsymbol{0} & = 
%- \frac{1}{|T|} 
%\left (
%\boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) 
%\boldsymbol{1}^\top 
%+ \left (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  
%\boldsymbol{1}^\top \right )^\top
%+ \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\Xi}}
%\right )
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{V}}(\boldsymbol{\lambda})
% \\
%& \qquad 
%+ \lambda_0 \hat{\boldsymbol{U}}(\boldsymbol{\lambda}) sign(\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})) 
%+ \epsilon \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda}) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})\\
%\end{align}
%\begin{align}
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\Xi}}(\boldsymbol{\lambda}) & = 
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{U}}(\boldsymbol{\lambda})
%\end{align}

\subsection{Backtracking Line Search}

Let the criterion function be $L:\mathbb{R}^n \rightarrow \mathbb{R}$. Suppose that the descent algorithm is currently at point $x$ with descent direction $\Delta x$. Backtracking line search uses a heuristic for finding a step size $t \in (0,1]$ such that the value of the criterion is minimized. The method depends on constants $\alpha  \in (0, 0.5)$ and $\beta \in (0,1)$.

\begin{algorithm}
\caption{Backtracking Line Search}
\label{alg:backtracking}
         \begin{algorithmic}
  	\STATE{Initialize $t= 1$.} \\
	\WHILE{$L(\boldsymbol x + t \boldsymbol \Delta \boldsymbol x) > L(\boldsymbol x) + \alpha t \nabla L(\boldsymbol x)^T \boldsymbol \Delta \boldsymbol x$}
	\STATE{Update $t := \beta t$}
	\ENDWHILE
	\end{algorithmic}
\end{algorithm}

\subsection{Simulation studies additional tables}

\subsubsection{Sparse add models}

\begin{table}
	\caption {\label{tab:additive} Additive models fitted with two penalty parameters, tuned by gradient descent, Nelder-Mead, and Spearmint. Standard errors are given in parentheses.}
	\centering
	\begin{tabular}{| l | l | l | l | l | }
		\hline
		& Num $\lambda$ & Validation Error & Test Error & \# Solves\\
		\hline
		% bayes: script_out/sparse_hc0_new_numsolves.out
		Gradient Descent & 2 & 23.87 (0.97) & 26.10 (0.86) & 13.07 \\
		\hline
		% bayes: script_out/sparse_nm0.out
		Nelder-Mead & 2 & 28.86 (1.04) & 29.97 (0.96) & 100 \\
		\hline
		% bayes: script_out/sparse_sp0.out
		Spearmint & 2 & 29.18 (1.07) & 30.09 (1.08) & 100 \\
		\hline
	\end{tabular}
\end{table}

To assess how sensitive gradient descent is to its initialization points, we tried multiple starting points for a smaller problem with 60 training, 30 validation, and 30 test observations and $p = 15$ covariates. The response was generated from \eqref{eq:simulation_sparse_add}, so the first three functions are non-zero and the remaining 12 are zero. We initialized $\boldsymbol{\lambda}$ by considering all possible combinations of $(\lambda_0, \lambda_1 \boldsymbol{1})$ where $\lambda_0, \lambda_1 \in \{10^i: i\in\{-2, ..., 1\}\}$ \textcolor{red}{(do you mean -2,-1,0,1? probably don't need ... for that)}. The initializations were randomly shuffled \textcolor{red}{(what does this mean?}. The results are shown in Figure \ref{fig:mult_starts}. For gradient descent, the validation and test error decreased with increasing number of observations, but there are no changes after the fourth initialization point. For Nelder-mead, the validation and test error continue to change as initializations increase. Even though the validation error decreases, the test error actually increases. This is probably due to the fact that Nelder-mead is unable to search the penalty parameter space effectively high-dimensions. Hence it essentially selects penalty parameters at random \textcolor{red}{I don't understand what you mean by this? Do you mean, once it is initialized it basically moves randomly? I still don't understand why test error will increase? It seems like decreasing validation error (even poorly) should generally decrease test error. I wonder if talking about the test error here is confusing}. From Figure \ref{fig:mult_starts} (Right), we also see that gradient descent fits models with lower validation and test error on average compared to Nelder-Mead \textcolor{red}{(the test error here seems wonky --- I think I might just look at validation error rather than test error)}.

\begin{figure}
	\label{fig:mult_starts}
	\caption{Error of additive models tuned by gradient descent vs. Nelder-mead. Left: Validation and test error of models after as the number of initialization points increases. Right: The distribution of validation and test errors.}
	\centering
	\includegraphics[width=0.55\textwidth]{many_inits_3_12_60_30_30_2_16_trend.png}
	\includegraphics[width=0.41\textwidth]{many_inits_3_12_60_30_30_2_16_box.png}
\end{figure}

\subsubsection{Un-pooled Sparse group lasso}
Table \ref{table:two_param_sgl} displays results from fitting the two-parameter version of the joint optimization problem \eqref{eq:unpooled_sgl} using gradient descent, Nelder-Mead, and Spearmint. Comparing the results in \label{table:unpooled}, we see that all four methods give similar the validation and test errors when tuning the model with two penalty parameters. Therefore regardless of the method used to tune the two-parameter sparse group lasso, the un-pooled sparse group lasso gives models with significantly lower test error \textcolor{red}{I think you need to say something more here; this seems like the point is a referee response, but no one else reading the appendix knows what the referee reports say!}.
\begin{table}
\caption{\label{table:two_param_sgl} Sparse Group Lasso fitted with two penalty parameters. Standard errors are given in parentheses. We abbreviated the methods as follows: Gradient Descent = GD, Nelder-Mead = NM, Spearmint = SP}
\centering
\begin{tabular}{| l | l | l | l | l | l | }
	\hline
	\multicolumn{6}{|c|}{n=90, p=600, M=30}\\
	\hline
	& \# $\lambda$ & $\beta$ Error & Validation Err & Test Err & \# Solves \\
	\hline
	% bayes: script_out/sgl_hc0_1.txt
	GD & 2 & 7.37 (0.18) & 46.82 (2.21) & 49.33 (1.36)& 21.43\\
	\hline
	% bayes: script_out/sgl_nm0_1_90_30_p600.out
	NM & 2 & 7.31 (0.18)  & 46.37 (2.24) & 48.95 (1.35) & 100 \\
	\hline
	% bayes: script_out/sgl_sp0_1.out
	SP & 2 & 7.35 (0.20) &  45.70 (2.32) & 49.35 (1.56) & 100 \\
	\hline
	\multicolumn{6}{|c|}{n=90, p=900, M=60}\\
	\hline
	& \# $\lambda$ & $\beta$ Error & Validation Error & Test Error & \# Solves \\
	\hline
	% bayes: script_out/sgl_hc0_2.out
	GD & 2 & 7.58 (0.21) & 45.71 (2.26) & 50.31 (1.93) & 20.77\\
	\hline
	% bayes: script_out/sgl_nm0_2.out
	NM & 2 & 7.56 (0.19) & 44.95 (2.24) & 50.18 (1.82) & 100  \\
	\hline
	% bayes: script_out/sgl_sp0_2.out
	SP & 2 & 8.20 (0.20)  & 49.59 (2.27) & 56.54 (2.14) & 100 \\
	\hline
	\multicolumn{6}{|c|}{n=90, p=1200, M=100}\\
	\hline
	& \# $\lambda$ & $\beta$ Error & Validation Error & Test Error & \# Solves \\
	\hline
	% bayes: script_out/sgl_hc0_3.out
	GD & 2 & 8.27 (0.19) & 50.46 (2.30) & 57.02 (1.94) & 19.80 \\
	\hline
	% bayes: script_out/sgl_nm0_3.out
	NM & 2 & 8.09 (0.19) & 49.92 (2.33) & 55.46 (1.89) & 100 \\
	\hline
	% bayes: script_out/sgl_sp0_3.out
	SP & 2 & 8.20 (0.20) & 49.70 (2.26) & 56.51 (2.16) & 100 \\
	\hline
\end{tabular}
\end{table}

\subsubsection{Low-rank Matrix Completion}
\textcolor{red}{I'm confused on what this section is trying to get across}. The results from fitting the two-parameter version of the joint optimization problem \eqref{eq:matrix_comp_groups} using gradient descent, Nelder-Mead, and Spearmint are displayed in Table \ref{table:two_param_matrix_completion}. Comparing these results to those in Table \label{table:matrix_completion} \textcolor{red}{I think this citation is having an issue?}, we see that these methods produce similar validation and test errors as grid search. More importantly, these results show that having separate penalty parameters for each of the covariate groups results in lower test error, regardless of the method used to tune the two-parameter joint optimization problem \textcolor{red}{(I thought in the last sentence you were saying they were the same as grid search?)}.

\begin{table}
	\caption{\label{table:two_param_matrix_completion} Matrix Completion fitted with two penalty parameters. Standard errors are given in parentheses. We abbreviated the methods as follows: Gradient Descent = GD, Nelder-Mead = NM, Spearmint = SP}
	\centering
	\begin{tabular}{| l | l | l | l | l |}
		\hline
		& \# $\lambda$ & Validation Err & Test Err &  Num Solves\\
		\hline
		% bayes: script_out/matrix_completion_groups_HC0_11_new2.txt
		GD & 2 & 0.70 (0.04) &  0.71 (0.04) & 8.03 (0.79) \\
		\hline
		% bayes: script_out/matrix_completion_groups_NM0_11_new1.txt
		NM & 2 & 0.71 (0.04) & 0.71 (0.04) & 100 \\
		\hline
		% bayes: script_out/matrix_completion_groups_SP0_11_new.txt
		SP & 2 & 0.73 (0.04) & 0.74 (0.04) & 100\\
		\hline
	\end{tabular}
\end{table}

\bibliographystyle{agsm}
\bibliography{hillclimbing_nonsmooth_appendix}

\end{document}
