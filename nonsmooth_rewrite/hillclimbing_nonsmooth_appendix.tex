\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{color}


% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

%%%% Packages and definitions
\usepackage{xr}
\externaldocument{hillclimbing_nonsmooth}

\usepackage[top=0.85in,left=1.0in,right=1.0in,footskip=0.75in]{geometry}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

\usepackage{algorithm,algorithmic}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

\usepackage{amsthm}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% for the beautiful checkmarks
\usepackage{pifont}

\DeclareMathOperator*{\argmin}{arg\,min}
%%%%

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\thealgorithm}{}

\appendix
\section{Appendix}

\subsection{$K$-fold Cross Validation}
We can perform joint optimization for $K$-fold cross validation by reformulating the problem. Let $(\boldsymbol y, \boldsymbol{X})$ be the full data set. We denote the $k$th fold as $(\boldsymbol y_{k}, \boldsymbol{X}_{k})$ and its complement as $(\boldsymbol y_{-k}, \boldsymbol{X}_{-k})$. Then the objective of this joint optimization problem is the average validation cost across all $K$ folds:
\begin{equation}
\begin{array}{c}
\argmin_{\boldsymbol{\lambda} \in \Lambda} \frac{1}{K} \sum_{k=1}^K L(\boldsymbol{y}_{k}, f_{\hat{\boldsymbol \theta}^{(k)}(\boldsymbol{\lambda})}(\boldsymbol{X}_k)) \\
\text{s.t. } {\hat{\boldsymbol \theta}^{(k)}(\boldsymbol{\lambda})} = \argmin_{\boldsymbol \theta \in \Theta} L(\boldsymbol{y}_{-k}, f_{\boldsymbol \theta} (\boldsymbol{X}_{-k})) + \sum\limits_{i=1}^J \lambda_i P_i(\boldsymbol \theta) \text{ for } k=1,...,K
\end{array}
\label{jointoptFullCV}
\end{equation}

\subsection{Proof of Theorem~\ref{thethrm}}

\begin{proof}
We will show that for a given $\boldsymbol \lambda_0$ that satisfies the given conditions, the validation loss is continuously differentiable within some neighborhood of $\boldsymbol \lambda_0$.  It then follows that if the theorem conditions hold true for almost every $\boldsymbol \lambda$, then the validation loss is continuously differentiable with respect to $\boldsymbol \lambda$ at almost every $\boldsymbol \lambda$.

Suppose the theorem conditions are satisfied at $\boldsymbol \lambda_0$. Let $\boldsymbol B'$ be an orthonormal set of basis vectors that span the differentiable space $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$ with the subset of vectors $\boldsymbol B$ that span the model parameter space.

Let $\tilde L_T(\boldsymbol \theta,\boldsymbol \lambda)$ be the gradient of $L_T(\cdot, \boldsymbol \lambda)$ at $\boldsymbol \theta$ with respect to the basis $\boldsymbol B$:
\begin{equation}
\tilde L_T(\boldsymbol \theta,\boldsymbol \lambda) = _{\boldsymbol B}\nabla L_T(\cdot, \boldsymbol \lambda) |_{\boldsymbol \theta}
\end{equation}

Since $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$ is the minimizer of the training loss, the gradient of $L_T(\cdot, \boldsymbol \lambda_0)$ with respect to the basis $\boldsymbol B$ must be zero at $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$:
\begin{equation}
_{\boldsymbol B}\nabla L_T(\cdot, \boldsymbol \lambda_0)|_{\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)} = \tilde L_T(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0) = 0
\end{equation}

From our assumptions, we know that there exists a neighborhood $W$ containing $\boldsymbol \lambda_0$ such that $\tilde L_T$ is continuously differentiable along directions in the differentiable space $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$. Also, the Jacobian matrix $D \tilde L_T(\cdot, \boldsymbol \lambda_0)|_{\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)}$ with respect to basis $\boldsymbol B$ is nonsingular. Therefore, by the implicit function theorem, there exist open sets $U \subseteq W$ containing $\boldsymbol \lambda_0$ and $V$ containing $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$ and a continuously differentiable function $\gamma: U \rightarrow V$ such that for every $\boldsymbol \lambda \in U$, we have that 
\begin{equation}
\tilde L_T(\gamma(\boldsymbol \lambda), \boldsymbol \lambda) = \nabla_{B} L_T(\cdot, \boldsymbol \lambda)|_{\gamma(\boldsymbol \lambda)} = 0
\end{equation}
That is, we know that $\gamma(\boldsymbol \lambda)$ is a continuously differentiable function that minimizes $L_T(\cdot, \boldsymbol \lambda)$ in the differentiable space  $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$.
Since we assumed that the differentiable space is a local optimality space of $L_T(\cdot, \boldsymbol \lambda)$ in the neighborhood $W$, then for every $\boldsymbol \lambda \in U$, 
\begin{equation}
\hat {\boldsymbol \theta}(\boldsymbol \lambda) =
\argmin_{\boldsymbol \theta} L_T(\boldsymbol \theta, \boldsymbol \lambda) =
\argmin_{\boldsymbol \theta \in \Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)} L_T(\boldsymbol \theta, \boldsymbol \lambda) =
\gamma(\boldsymbol \lambda)
\end{equation}
Therefore, we have shown that if $\boldsymbol \lambda_0$ satisfies the assumptions given in the theorem, the fitted model parameters $\hat {\boldsymbol \theta}(\boldsymbol \lambda)$ is a continuously differentiable function within a neighborhood of $\boldsymbol \lambda_0$. We can then apply the chain rule to get the gradient of the validation loss.
\end{proof}

\subsection{Regression Examples}

\subsubsection{Elastic Net}\label{enet_conditions}
We show that the joint optimization problem for the Elastic Net satisfies all three conditions in Theorem~\ref{thethrm}:
\begin{itemize}
\item[] Condition 1: The elastic net solution paths are piecewise linear \citep{zou2003regression}, which means that the nonzero indices of the elastic net estimates stay locally constant for almost every $\boldsymbol{\lambda}$. Therefore, $S_{\boldsymbol{\lambda}}$ as defined in Section~\ref{sec:enet}  is a local optimality space for $L_T(\cdot, \boldsymbol{\lambda})$. \hfill \ding{51}
\item[] Condition 2: We only need to establish that the $\ell_1$ penalty is twice-continuously differentiable in the directions of $S_{\boldsymbol{\lambda}}$ since the quadratic loss function and the ridge penalty are both smooth. The absolute value function is twice-continuously differentiable everywhere except at zero. Hence the training criterion is smooth when restricted to $S_{\boldsymbol{\lambda}}$. \hfill \ding{51}
\item[] Condition 3: The Hessian matrix of $L_T(\cdot, \boldsymbol{\lambda})$ with respect to $\boldsymbol I_{I(\boldsymbol \lambda)}$ is $\boldsymbol I_{I(\boldsymbol \lambda)}^\top \boldsymbol{X}_{T}^\top \boldsymbol{X}_{T} \boldsymbol I_{I(\boldsymbol \lambda)} + \lambda_2 \boldsymbol{I}$. The first summand is positive semi-definite. As long as $\lambda_2 > 0$, the contribution of the identity matrix ensures the Hessian is positive definite. \hfill \ding{51}
\end{itemize}

\subsubsection{Additive Models with Sparsity and Smoothness Penalties}
\label{sec_appendix:sparse_add_models}
Let 
\begin{equation}
\boldsymbol{U} = \begin{bmatrix}
\boldsymbol {U}^{(i_1)} & ... & \boldsymbol {U}^{(i_{|J(\boldsymbol \lambda)|})}
\end{bmatrix}
\end{equation}
where $i_\ell \in J(\boldsymbol \lambda)$. Then the Hessian matrix in this problem is
\begin{equation}
\boldsymbol{H}(\boldsymbol\lambda)
= \boldsymbol{U}^\top \boldsymbol I_T^\top \boldsymbol I_T \boldsymbol{U}
+ \lambda_0 \text{ } diag \left (
\frac{1}{||\boldsymbol {U}^{(i)}  \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)||_2} \left (
\boldsymbol I - \frac{\hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)^\top \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)}{||\boldsymbol {U}^{(i)}  \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)||_2}
\right ) \right )
+ \epsilon \boldsymbol I
\label{eq:add_hessian}
\end{equation}
Now we check that all three conditions are satisfied. 
\begin{itemize}
	\item[] Condition 1: It seems likely that the space spanned by $S_{\boldsymbol{\lambda}}$ is a local optimality space, though we are unable to formally prove this fact. The training criterion for this problem is composed of generalized lasso penalties and a group lasso penalties. For the generalized lasso, \citet{tibshirani2011solution} proved that the solution path is smooth almost everywhere. For the group lasso, there is empirical evidence that the active set is locally constant almost everywhere with respect to the penalty parameter \citep{yuan2006model}, but this has not been formally proven. Nonetheless, is a related result that the active set is locally constant with respect to the response \citep{vaiter2012degrees}; we suspect similar techniques could be used to prove our desired result.
	% However, the dual formulation of this problem suggests that this condition holds. In particular, if we create dummy constraints $\boldsymbol{z}^{(i)} = \boldsymbol{I}_T\boldsymbol{\theta}^{(i)}$ and $\boldsymbol{w}^{(i)} = \boldsymbol{D}_{x_i}^{(2)} \boldsymbol{\theta}^{(i)}$ with corresponding dual variables $\boldsymbol{u}^{(i)}$ and $\boldsymbol{v}^{(i)}$, we find that the dual variables must satisfy constraints of the form $\|\boldsymbol{I}_T^\top \boldsymbol{u}^{(i)}\|_2 \le \lambda_0$ and $\|\boldsymbol{v}^{(i)}\|_\infty \le \lambda_i$. It therefore seems likely that the dual solution perturbs smoothly with $\boldsymbol{\lambda}$. Under some regularity conditions, there is also a smooth mapping between the primal and dual solutions. Hence the fitted model probably smooth with respect to $\boldsymbol{\lambda}$. Finally, assuming that the space spanned by the nonzero coefficients of $\hat{\boldsymbol{\theta}}\boldsymbol{\lambda}$ is a local optimality space, then clearly the local optimality space is also a differentiable space.
	\item[] Condition 2:  We only need to establish that the generalized lasso and group lasso penalties are twice-continuously differentiable in the directions of $S_{\boldsymbol{\lambda}}$ since the rest of the training criterion is smooth. 
	$\| \boldsymbol{D} \boldsymbol{\theta} \|_1$ is not differentiable at the points where $\boldsymbol{D} \boldsymbol{\theta}$ has zero elements. We must therefore restrict the derivatives to be taken in directions such that the zero elements of $\boldsymbol{D} \boldsymbol{\theta}$ remain constant. The $\ell_2$ norm is twice-continuously differentiable everywhere except at the zero vector. Hence the training criterion is smooth when restricted to the differentiable space $S_{\boldsymbol{\lambda}}$ specified in Section~\ref{sec:additive}.
	\hfill \ding{51}
	\item[] Condition 3: The Hessian matrix in \eqref{eq:add_hessian} is the sum of positive semi-definite matrices. As long as $\epsilon > 0$, the contribution of the last summand $\epsilon \boldsymbol{I}$ will make the Hessian matrix positive-definite. \hfill \ding{51}
\end{itemize}

In the gradient calculation for this problem, the matrix $\boldsymbol C(\boldsymbol \beta( \boldsymbol \lambda))$ in \eqref{eq:additive_gradient} has columns $i = 1,...,p$
\begin{equation}
C_i(\boldsymbol \beta( \boldsymbol \lambda))
= \begin{cases}
\begin{bmatrix}
\boldsymbol{0} \\
\boldsymbol {U}^{(i)\top}  \boldsymbol{D}^{(2)\top}_{\boldsymbol{x}_i} 
sgn( \boldsymbol{D}^{(2)}_{\boldsymbol{x}_i} \boldsymbol {U}^{(i)} \hat{\boldsymbol{\beta}}^{(i)}) \\
\boldsymbol{0} \\
\end{bmatrix}
& \text{ for } i \in J(\boldsymbol \lambda) \\
\boldsymbol{0}
& \text{ for } i \not\in J(\boldsymbol \lambda) \\
\end{cases}
\end{equation}


\subsubsection{Un-pooled Sparse Group Lasso}
The Hessian in this problem is
\begin{equation}
\boldsymbol{H}(\boldsymbol\lambda) =
\frac{1}{n} \boldsymbol X_{T, I(\boldsymbol \lambda)}^\top \boldsymbol X_{T, I(\boldsymbol \lambda)}
+ diag\left(
\frac{\lambda_m}{|| \boldsymbol \theta^{(m)}||_2}
\left (
\boldsymbol I - 
\frac{\boldsymbol \theta^{(m)} \boldsymbol \theta^{(m) \top}}{|| \boldsymbol \theta^{(m)}||_2^2}
\right )
\right)
+ \epsilon \boldsymbol I
\label{eq:sgl_hessian}
\end{equation}
The logic for checking all three conditions in Theorem~\ref{thethrm} is similar to the other examples:
\begin{itemize}
	\item[] Condition 1: We hypothesize that the differentiable space $S_{\boldsymbol{\lambda}}$ is also a local optimality space, though we have not formally proven this fact. We suspect this is true for the same reasons discussed in Section \ref{sec_appendix:sparse_add_models}.
	%it is difficult to formally prove that the local optimality space at $\boldsymbol{\lambda}$ is the space spanned by the nonzero coefficients of $\boldsymbol{\theta}$. However if we follow very similar reasoning, the dual formulation of this problem suggests that the fitted model perturbs smoothly in $\boldsymbol{\lambda}$ almost everywhere. Hence it seems likely that the set of nonzero coefficients $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$ is constant at almost every $\boldsymbol{\lambda}$. Assuming this is true, then the local optimality space $S_{\boldsymbol{\lambda}}$ is clearly also a differentiable space.
	\item[] Condition 2: The $\ell_1$ and $\ell_2$ penalties are twice-differentiable when restricted to $S_{\boldsymbol{\lambda}}$ for the same reasons discussed in Section~\ref{sec_appendix:sparse_add_models}. \hfill \ding{51}
	\item[] Condition 3: The Hessian matrix in \eqref{eq:sgl_hessian} is the sum of positive semi-definite matrices. It is positive definite for any $\epsilon > 0$ due to the contribution from the last summand $\epsilon \boldsymbol{I}$. \hfill \ding{51}
\end{itemize}

In the gradient calculations for this problem, the matrix $\boldsymbol C(\hat {\boldsymbol \beta}(\boldsymbol \lambda))$ in \eqref{eq:unpooled_sgl_grad} has columns $m=1,2...,M$ 
\begin{equation}
C_i(\hat{\boldsymbol \beta}( \boldsymbol \lambda))
=
\begin{bmatrix}
\boldsymbol 0\\
\frac{\hat {\boldsymbol \beta}^{(m)}(\boldsymbol \lambda)}{||\hat{\boldsymbol \beta}^{(m)}(\boldsymbol \lambda)||_2}\\
\boldsymbol 0\\
\end{bmatrix}
\end{equation}
where $\boldsymbol 0$ are the appropriate dimensions.

\subsubsection{Low-rank Matrix Completion}
We first derive a differentiable space of the training criterion \eqref{eq:matrix_comp_groups} with respect to $\boldsymbol{\Gamma}$. 
Suppose $\boldsymbol{\Gamma} = \boldsymbol{U}diag(\boldsymbol{\sigma}) \boldsymbol{V}^\top$ is the singular value decomposition of $\boldsymbol{\Gamma}$ where the $\boldsymbol{\sigma}$ is a vector of the singular values in non-increasing order. The subdifferential of the nuclear norm is \citep{parikh2014proximal}
\begin{equation}
\partial \| \boldsymbol{\Gamma} \|_* = 
\left \{
\boldsymbol{U} \text{diag}(\boldsymbol{\mu}) \boldsymbol{V}^\top \middle | 
\mu_i \in 
\begin{cases}
[-1, 1] & \text{if } \sigma_i = 0\\
\text{sign}(\sigma_i) & \text{if } \sigma_i \ne 0\\
\end{cases}
\right \}
\end{equation}
The subdifferential reduces to a gradient if we restrict the derivative to be taken in the directions
\begin{align}
S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}} & =  \left \{
\boldsymbol{G} \in \mathbb{R}^{N\times N}
\middle |
\text{range}(\boldsymbol{G}) \subseteq \text{range}(\boldsymbol{\Gamma})
\right \} \\
& = 
\text{span}
\left (
\left \{
\boldsymbol{B}^{(ij)} = \boldsymbol{u}_j \boldsymbol{e}_i^\top
\middle | 
i,j  \in \{1,...,N\},
\sigma_j \ne 0
\right \}
\right )
\label{eq:lin_combo_diff_space}
\end{align}
where $\boldsymbol{u}_i$ is the $i$th column of $\boldsymbol{U}$ and $\boldsymbol{e}_i \in \mathbb{R}^N$ is the $i$th standard basis vector. The matrices in \eqref{eq:lin_combo_diff_space} form an orthonormal basis of $S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}}$.

By reparameterizing the matrix $\boldsymbol{\Gamma} = \sum_{i=1}^N \sum_{j : \sigma_j \ne 0} b_{ij} \boldsymbol{B}^{(ij)}$, the derivative of the nuclear norm with respect to the basis $\boldsymbol{B}$ simplifies to
\begin{equation}
_{\boldsymbol{B}}\nabla 
\left \| \sum_{i=1}^N \sum_{j : \sigma_j \ne 0} b_{ij} \boldsymbol{B}^{(ij)} 
\right \|_{*} = 
\text{sign}(\boldsymbol{b})
\end{equation}
%$S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}}$ is a linear combination of a fixed set of matrices $\boldsymbol{u}_j \boldsymbol{e}_i^\top$, there exists some orthonormal basis of \eqref{eq:lin_combo_diff_space}. Hence we can write the differentiable space with respect to $\boldsymbol{\Gamma}$ as \eqref{eq:nuclear_norm_diff_space}. 
$S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}}$ can also be interpreted as the set of matrix directions that preserve the rank:
\begin{equation}
S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}} = \{\boldsymbol{G} | \text{rank}(\boldsymbol{\Gamma} + \delta \boldsymbol{G}) \le \text{rank}(\boldsymbol{\Gamma}), \delta \in [0,1] \}
\label{eq:rank_preserve_diff_space}
\end{equation}
The basis representation of $S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}}$ \eqref{eq:nuclear_norm_diff_space} is useful for proving the conditions are satisfied for Theorem~\ref{thethrm} whereas the rank-preserving representation \eqref{eq:rank_preserve_diff_space} is useful for calculating the gradient.

We can now determine the gradient of the validation loss. Minimize with respect to $\boldsymbol{b}$
\begin{equation}
\begin{array}{c}
\frac{1}{2|T|} 
\left \| 
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \boldsymbol{\eta} \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \boldsymbol{\gamma}  \boldsymbol{1}^\top )^\top
- \sum_{i=1}^N \sum_{j : \sigma_j \ne 0} b_{ij} \boldsymbol{B}^{(ij)}
\right \|^2_T
+ \lambda_0 \| \sum_{i=1}^N \sum_{j : \sigma_j \ne 0} b_{ij} \boldsymbol{B}^{(ij)} \|_*
\\
+ \sum_{g=1}^G  \lambda_g \| \boldsymbol\eta^{(g)} \|_2
+ \sum_{g=1}^G  \lambda_{G+g} \| \boldsymbol\gamma^{(g)} \|_2
+ \frac{1}{2} \epsilon \left (
\| \boldsymbol\eta \|_2^2 + \| \boldsymbol\gamma \|_2^2 
+ \| \sum_{i=1}^N \sum_{j : \sigma_j \ne 0} b_{ij} \boldsymbol{B}^{(ij)}\|^2_F
\right )
\end{array}
\end{equation}
Take gradient of training criterion with respect to $b_{k\ell}$
\begin{equation}
\begin{array}{c}
0 =
- \frac{1}{|T|} 
\boldsymbol{B}^{(k\ell)\top}
\left (
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  \boldsymbol{1}^\top )^\top
- \sum_{i=1}^N \sum_{j : \sigma_j \ne 0} \hat{b}_{ij}(\boldsymbol{\lambda}) \boldsymbol{B}^{(ij)}
\right )\\
+ \lambda_0 \text{sign}(
\hat{b}_{k\ell}(\boldsymbol{\lambda})
)
+ \epsilon \hat{b}_{k\ell}(\boldsymbol{\lambda})
\end{array}
\end{equation}
%%%%%%%%%%%%%%%% OLD?
In the other examples, we derived the gradient by re-parameterizing all the variables with respect to this basis. The same approach could be applied here, but it requires determining the orthonormal basis of $S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}}$. Instead, we will construct a smooth training criterion by expressing $\boldsymbol{\Gamma}$ using its singular value decomposition. At $\boldsymbol{\lambda}$, the training criterion is smooth as long as we restrict the rank of $\boldsymbol{\Gamma}$ to be no larger than $\hat{\boldsymbol{\Gamma}}(\boldsymbol{\lambda})$. We can therefore re-parameterize $\boldsymbol{\Gamma}$ using its compact singular value decomposition $\boldsymbol{U} \text{diag}(\boldsymbol{\sigma}) \boldsymbol{V}^\top$ where $\boldsymbol{U} \in  \mathbb{R}^{N \times \hat{r}}, \boldsymbol{\sigma} \in  \mathbb{R}^{\hat{r}}$ and $\boldsymbol{V} \in  \mathbb{R}^{\hat{r} \times N}$ to get the following smooth training criterion:
\begin{equation}
\begin{array}{c}
\frac{1}{2|T|} 
\left \| 
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \boldsymbol{\eta} \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \boldsymbol{\gamma}  \boldsymbol{1}^\top )^\top
- \boldsymbol{U} \text{diag}(\boldsymbol{\sigma}) \boldsymbol{V}^\top
\right \|^2_T
+ \lambda_0 \| \boldsymbol{U} \text{diag}(\boldsymbol{\sigma}) \boldsymbol{V}^\top \|_*
 \\
+ \sum_{g=1}^G  \lambda_g \| \boldsymbol\eta^{(g)} \|_2
+ \sum_{g=1}^G  \lambda_{G+g} \| \boldsymbol\gamma^{(g)} \|_2
+ \frac{1}{2} \epsilon \left (
\| \boldsymbol\eta \|_2^2 + \| \boldsymbol\gamma \|_2^2 
+ \|\boldsymbol{U} \text{diag}(\boldsymbol{\sigma}) \boldsymbol{V}^\top\|^2_F
\right )
\\
\text{s.t. } \boldsymbol{V}^\top \boldsymbol{V} = \boldsymbol{I}
\text{ and } \boldsymbol{U}^\top \boldsymbol{U} = \boldsymbol{I}
\end{array}
\end{equation}
We can take the subgradient of the training criterion with respect to $\boldsymbol{\Gamma} = \boldsymbol{U} diag(\boldsymbol{\sigma}) \boldsymbol{V}^\top$ and multiply it by its left singular vectors to get the following gradient optimality condition:
\begin{align}
\begin{split}
\boldsymbol{0} & = 
- \frac{1}{|T|} 
\hat{\boldsymbol{U}}(\boldsymbol{\lambda})^\top
\left (
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  \boldsymbol{1}^\top )^\top
\right )
+ \frac{1}{|T|} \text{diag}(\hat{\boldsymbol{\sigma}}(\boldsymbol{\lambda})) \hat{\boldsymbol{V}}^\top(\boldsymbol{\lambda})
\\
& \qquad + \lambda_0 \text{diag}(\text{sign}(\hat{\boldsymbol{\sigma}}(\boldsymbol{\lambda}))) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
+ \epsilon diag(\hat{\boldsymbol{\sigma}}(\boldsymbol{\lambda})) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
\end{split}
\label{eq:grad_opt_matrix_left}
\end{align}
Similarly, we multiply the result by its right singular vectors to get
\begin{align}
\begin{split}
\boldsymbol{0} & = - \frac{1}{|T|} 
\left (
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  \boldsymbol{1}^\top )^\top
\right )
\hat{\boldsymbol{V}}(\boldsymbol{\lambda})
+ \frac{1}{|T|} 
\hat{\boldsymbol{U}}(\boldsymbol{\lambda})
\text{diag}(\hat{\boldsymbol{\sigma}}(\boldsymbol{\lambda})) 
\\
& \qquad + \lambda_0 \hat{\boldsymbol{U}}(\boldsymbol{\lambda}) \text{diag}(\text{sign}(\hat{\boldsymbol{\sigma}}(\boldsymbol{\lambda})))
+ \epsilon \hat{\boldsymbol{U}}(\boldsymbol{\lambda}) \text{diag}(\hat{\boldsymbol{\sigma}}(\boldsymbol{\lambda}))
\end{split}
\label{eq:grad_opt_matrix_comp}
\end{align}
To get the gradient, we then implicitly differentiate \eqref{eq:grad_opt_matrix_left} and \eqref{eq:grad_opt_matrix_comp} with respect to $\boldsymbol{\lambda}$.
We also implicitly differentiate the conditions $\boldsymbol{\hat U}^\top(\boldsymbol{\lambda}) \boldsymbol{\hat{U}}(\boldsymbol{\lambda}) = \boldsymbol{I}$ and $\boldsymbol{\hat{V}}(\boldsymbol{\lambda})^\top \boldsymbol{\hat{V}}(\boldsymbol{\lambda}) = \boldsymbol{I}$ with respect to $\boldsymbol{\lambda}$ to get additional linear constraints. Combining these constraints with those from the gradient optimality conditions with respect to $\boldsymbol{\eta}$ and $\boldsymbol{\gamma}$, we can solve for the gradient of the validation loss with respect to $\boldsymbol{\lambda}$.

Finally, we show that the conditions in Theorem~\ref{thethrm} are satisfied.
%we cannot apply Theorem~\ref{thethrm} directly to the problem since we use the modified gradient optimality conditions \eqref{eq:grad_opt_matrix_left} and \eqref{eq:grad_opt_matrix_comp}. This requires a straightforward modification of Theorem~\ref{thethrm} and its proof. Condition 1 remains the same. Condition 2 requires the expressions in \eqref{eq:grad_opt_matrix_left} and \eqref{eq:grad_opt_matrix_comp} to be continuously differentiable with respect to $\boldsymbol{U}, \boldsymbol{V}, \boldsymbol{\Sigma}, \boldsymbol{\eta}$ and $\boldsymbol{\gamma}$. Condition 3 requires that that the partial derivative of the vectorized gradient optimality conditions with respect to $\boldsymbol{U}, \boldsymbol{V}, \boldsymbol{\Sigma}, \boldsymbol{\eta}$ and $\boldsymbol{\gamma}$ to be a nonsingular matrix.

\begin{itemize}
	\item[] Condition 1: We hypothesize that the differentiable space $S_{\boldsymbol{\lambda}}$ defined in \eqref{eq:matrix_completion_diff_space} is also a local optimality space $\boldsymbol{\lambda}$. For the group lasso penalties, we use the same reasons mentioned in \ref{sec_appendix:sparse_add_models} to justify this hypothesis. For the nuclear norm penalty, it has been observed empirically that small perturbations in the penalty parameter result in matrices with same rank \citep{mazumder2010spectral}. This supports our belief that $S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}}$ is a local optimality space with respect to $\boldsymbol{\Gamma}$ at $\boldsymbol{\lambda}$.
	%is the space spanned by the nonzero coefficients of $\boldsymbol{\theta}$. However if we follow very similar reasoning, the dual formulation of this problem suggests that the fitted model perturbs smoothly in $\boldsymbol{\lambda}$ almost everywhere. (Note that in the dual formulation, the constraints are now with respect to the spectral norm.) Hence it seems likely that the rank of the matrix stays constant at almost every $\boldsymbol{\lambda}$. Assuming this is true, then the local optimality space $S_{\boldsymbol{\lambda}}$ is clearly also a differentiable space.
	\item[] Condition 2: The only non-smooth components of the training criterion are the group lasso and nuclear norm penalties. The group lasso penalty is twice-differentiable when restricted to the differentiable space, using the same reasoning in Section~\ref{sec_appendix:sparse_add_models}. The nuclear norm $\|\boldsymbol{\Gamma}\|_{*}$ is twice differentiable with respect to $\boldsymbol{\Gamma}$ when restricted to $S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}}$, as proven in this section.
	\hfill \ding{51}
	\item[] Condition 3: We showed in \eqref{eq:lin_combo_diff_space} that the differentiable space with respect to $\boldsymbol{\Gamma}$ at $\boldsymbol{\lambda}$ is a vector space with some orthonormal basis. Therefore there is an orthonormal basis $\boldsymbol{B}$ such that the Hessian of the training criterion with respect to $\boldsymbol{B}$ exists. The training criterion is the sum of convex functions, including ridge penalties for all the variables. As long as the ridge penalties are scaled by some $\epsilon > 0$, the Hessian of the training criterion is positive definite.
	\hfill \ding{51}
\end{itemize}



%\begin{align}
%\boldsymbol{0} & = 
%\frac{1}{|T|} 
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{U}}(\boldsymbol{\lambda})^\top
%\left (
%\boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) 
%\boldsymbol{1}^\top 
%+ \left (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  
%\boldsymbol{1}^\top \right )^\top
%+ \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\Xi}}
%\right ) \\
%& \qquad \lambda_0 sign(\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
%+ \epsilon \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda}) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top\\
%% next equation
%\boldsymbol{0} & = 
%- \frac{1}{|T|} 
%\left (
%\boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) 
%\boldsymbol{1}^\top 
%+ \left (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  
%\boldsymbol{1}^\top \right )^\top
%+ \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\Xi}}
%\right )
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{V}}(\boldsymbol{\lambda})
% \\
%& \qquad 
%+ \lambda_0 \hat{\boldsymbol{U}}(\boldsymbol{\lambda}) sign(\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})) 
%+ \epsilon \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda}) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})\\
%\end{align}
%\begin{align}
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\Xi}}(\boldsymbol{\lambda}) & = 
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{U}}(\boldsymbol{\lambda})
%\end{align}

\subsection{Backtracking Line Search}

Let the criterion function be $L:\mathbb{R}^n \rightarrow \mathbb{R}$. Suppose that the descent algorithm is currently at point $x$ with descent direction $\Delta x$. Backtracking line search uses a heuristic for finding a step size $t \in (0,1]$ such that the value of the criterion is minimized. The method depends on constants $\alpha  \in (0, 0.5)$ and $\beta \in (0,1)$.

\begin{algorithm}
\caption{Backtracking Line Search}
\label{alg:backtracking}
         \begin{algorithmic}
  	\STATE{Initialize $t= 1$.} \\
	\WHILE{$L(\boldsymbol x + t \boldsymbol \Delta \boldsymbol x) > L(\boldsymbol x) + \alpha t \nabla L(\boldsymbol x)^T \boldsymbol \Delta \boldsymbol x$}
	\STATE{Update $t := \beta t$}
	\ENDWHILE
	\end{algorithmic}
\end{algorithm}

\subsection{Simulation studies additional tables}

\subsubsection{Sparse add models}

\begin{table}
	\caption {\label{tab:additive} Additive models fitted with two penalty parameters, tuned by gradient descent, Nelder-Mead, and Spearmint. Standard errors are given in parentheses.}
	\centering
	\begin{tabular}{| l | l | l | l | l | }
		\hline
		& Num $\lambda$ & Validation Error & Test Error & \# Solves\\
		\hline
		% bayes: script_out/sparse_hc0_new_numsolves.out
		Gradient Descent & 2 & 23.87 (0.97) & 26.10 (0.86) & 13.07 \\
		\hline
		% bayes: script_out/sparse_nm0.out
		Nelder-Mead & 2 & 28.86 (1.04) & 29.97 (0.96) & 100 \\
		\hline
		% bayes: script_out/sparse_sp0.out
		Spearmint & 2 & 29.18 (1.07) & 30.09 (1.08) & 100 \\
		\hline
	\end{tabular}
\end{table}

To assess how sensitive gradient descent is to its initialization points, we tried multiple starting points for a smaller problem with 60 training, 30 validation, and 30 test observations and $p = 15$ covariates. The response was generated from \eqref{eq:simulation_sparse_add}, so the first three functions are non-zero and the remaining 12 are zero. We initialized $\boldsymbol{\lambda}$ by considering all possible combinations of $(\lambda_0, \lambda_1 \boldsymbol{1})$ where $\lambda_0, \lambda_1 \in \{10^i: i\in\{-2, -1, 0, 1\}\}$. The results are shown in Figure \ref{fig:mult_starts}. For gradient descent, the validation and test error decreased with increasing number of observations, but there are no changes after the fourth initialization point. For Nelder-mead, the validation and test error continue to change as initializations increase. Even though the validation error decreases, the test error actually increases. This is probably due to the fact that Nelder-mead is unable to search the penalty parameter space effectively high-dimensions. Hence it essentially selects penalty parameters at random \textcolor{red}{I don't understand what you mean by this? Do you mean, once it is initialized it basically moves randomly? I still don't understand why test error will increase? It seems like decreasing validation error (even poorly) should generally decrease test error. I wonder if talking about the test error here is confusing}. From Figure \ref{fig:mult_starts} (Right), we also see that gradient descent fits models with lower validation and test error on average compared to Nelder-Mead \textcolor{red}{(the test error here seems wonky --- I think I might just look at validation error rather than test error)}.

\begin{figure}
	\label{fig:mult_starts}
	\caption{Error of additive models tuned by gradient descent vs. Nelder-mead. Left: Validation error of models after as the number of initialization points increases. Right: The distribution of validation errors.
	\textcolor{red}{replace legend text to GD}
	}
	\centering
	\includegraphics[width=0.45\textwidth]{many_inits_3_12_60_30_30_2_16_trend.png}
	\includegraphics[width=0.45\textwidth]{many_inits_3_12_60_30_30_2_16_box.png}
\end{figure}

\subsubsection{Un-pooled Sparse group lasso}
Table \ref{table:two_param_sgl} displays results from fitting the two-parameter version of the joint optimization problem \eqref{eq:unpooled_sgl} using gradient descent, Nelder-Mead, and Spearmint. Comparing the results in \label{table:unpooled}, we see that all four methods give similar the validation and test errors when tuning the model with two penalty parameters. Therefore regardless of the method used to tune the two-parameter sparse group lasso, the un-pooled sparse group lasso gives models with significantly lower test error \textcolor{red}{I think you need to say something more here; this seems like the point is a referee response, but no one else reading the appendix knows what the referee reports say!}.
\begin{table}
\caption{\label{table:two_param_sgl} Sparse Group Lasso fitted with two penalty parameters. Standard errors are given in parentheses. We abbreviated the methods as follows: Gradient Descent = GD, Nelder-Mead = NM, Spearmint = SP}
\centering
\begin{tabular}{| l | l | l | l | l | l | }
	\hline
	\multicolumn{6}{|c|}{n=90, p=600, M=30}\\
	\hline
	& \# $\lambda$ & $\beta$ Error & Validation Err & Test Err & \# Solves \\
	\hline
	% bayes: script_out/sgl_hc0_1.txt
	GD & 2 & 7.37 (0.18) & 46.82 (2.21) & 49.33 (1.36)& 21.43\\
	\hline
	% bayes: script_out/sgl_nm0_1_90_30_p600.out
	NM & 2 & 7.31 (0.18)  & 46.37 (2.24) & 48.95 (1.35) & 100 \\
	\hline
	% bayes: script_out/sgl_sp0_1.out
	SP & 2 & 7.35 (0.20) &  45.70 (2.32) & 49.35 (1.56) & 100 \\
	\hline
	\multicolumn{6}{|c|}{n=90, p=900, M=60}\\
	\hline
	& \# $\lambda$ & $\beta$ Error & Validation Error & Test Error & \# Solves \\
	\hline
	% bayes: script_out/sgl_hc0_2.out
	GD & 2 & 7.58 (0.21) & 45.71 (2.26) & 50.31 (1.93) & 20.77\\
	\hline
	% bayes: script_out/sgl_nm0_2.out
	NM & 2 & 7.56 (0.19) & 44.95 (2.24) & 50.18 (1.82) & 100  \\
	\hline
	% bayes: script_out/sgl_sp0_2.out
	SP & 2 & 8.20 (0.20)  & 49.59 (2.27) & 56.54 (2.14) & 100 \\
	\hline
	\multicolumn{6}{|c|}{n=90, p=1200, M=100}\\
	\hline
	& \# $\lambda$ & $\beta$ Error & Validation Error & Test Error & \# Solves \\
	\hline
	% bayes: script_out/sgl_hc0_3.out
	GD & 2 & 8.27 (0.19) & 50.46 (2.30) & 57.02 (1.94) & 19.80 \\
	\hline
	% bayes: script_out/sgl_nm0_3.out
	NM & 2 & 8.09 (0.19) & 49.92 (2.33) & 55.46 (1.89) & 100 \\
	\hline
	% bayes: script_out/sgl_sp0_3.out
	SP & 2 & 8.20 (0.20) & 49.70 (2.26) & 56.51 (2.16) & 100 \\
	\hline
\end{tabular}
\end{table}

\subsubsection{Low-rank Matrix Completion}
\textcolor{red}{I'm confused on what this section is trying to get across}. The results from fitting the two-parameter version of the joint optimization problem \eqref{eq:matrix_comp_groups} using gradient descent, Nelder-Mead, and Spearmint are displayed in Table \ref{table:two_param_matrix_completion}. Comparing these results to those in Table \label{table:matrix_completion} \textcolor{red}{I think this citation is having an issue?}, we see that these methods produce similar validation and test errors as grid search. More importantly, these results show that having separate penalty parameters for each of the covariate groups results in lower test error, regardless of the method used to tune the two-parameter joint optimization problem \textcolor{red}{(I thought in the last sentence you were saying they were the same as grid search?)}.

\begin{table}
	\caption{\label{table:two_param_matrix_completion} Matrix Completion fitted with two penalty parameters. Standard errors are given in parentheses. We abbreviated the methods as follows: Gradient Descent = GD, Nelder-Mead = NM, Spearmint = SP}
	\centering
	\begin{tabular}{| l | l | l | l | l |}
		\hline
		& \# $\lambda$ & Validation Err & Test Err &  Num Solves\\
		\hline
		% bayes: script_out/matrix_completion_groups_HC0_11_new2.txt
		GD & 2 & 0.70 (0.04) &  0.71 (0.04) & 8.03 (0.79) \\
		\hline
		% bayes: script_out/matrix_completion_groups_NM0_11_new1.txt
		NM & 2 & 0.71 (0.04) & 0.71 (0.04) & 100 \\
		\hline
		% bayes: script_out/matrix_completion_groups_SP0_11_new.txt
		SP & 2 & 0.73 (0.04) & 0.74 (0.04) & 100\\
		\hline
	\end{tabular}
\end{table}

\bibliographystyle{agsm}
\bibliography{hillclimbing_nonsmooth_appendix}

\end{document}
