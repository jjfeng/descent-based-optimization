\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{color}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

%%%% Packages and definitions
\usepackage{xr}
\externaldocument{hillclimbing_nonsmooth}
\usepackage{mathtools}
\usepackage[top=0.85in,left=1.0in,right=1.0in,footskip=0.75in]{geometry}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8]{inputenc}
\usepackage{caption}
% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

\usepackage[ruled]{algorithm}
\usepackage[noend]{algorithmic}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

\usepackage{amsthm}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

\DeclareMathOperator*{\argmin}{arg\,min}
\newtheorem{lemma}{Lemma}
%%%%


\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\thealgorithm}{}

\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}

\appendix

\section{Appendix}

\subsection{$K$-fold Cross Validation}
We can perform joint optimization for $K$-fold cross validation by reformulating the problem. Let $(\boldsymbol y, \boldsymbol{X})$ be the full data set. We denote the $k$th fold as $(\boldsymbol y_{k}, \boldsymbol{X}_{k})$ and its complement as $(\boldsymbol y_{-k}, \boldsymbol{X}_{-k})$. Then the objective of this joint optimization problem is the average validation cost across all $K$ folds:
\begin{equation}
\begin{array}{c}
\argmin_{\boldsymbol{\lambda} \in \Lambda} \frac{1}{K} \sum_{k=1}^K L(\boldsymbol{y}_{k}, f_{\hat{\boldsymbol \theta}^{(k)}(\boldsymbol{\lambda})}(\boldsymbol{X}_k)) \\
\text{s.t. } {\hat{\boldsymbol \theta}^{(k)}(\boldsymbol{\lambda})} = \argmin_{\boldsymbol \theta \in \Theta} L(\boldsymbol{y}_{-k}, f_{\boldsymbol \theta} (\boldsymbol{X}_{-k})) + \sum\limits_{i=1}^J \lambda_i P_i(\boldsymbol \theta) \text{ for } k=1,...,K
\end{array}
\label{jointoptFullCV}
\end{equation}

\subsection{Proof of Theorem~\ref{thethrm}}

\begin{proof}
We will show that for a given $\boldsymbol \lambda_0$ that satisfies the given conditions, the validation loss is continuously differentiable within some neighborhood of $\boldsymbol \lambda_0$.  It then follows that if the theorem conditions hold true for almost every $\boldsymbol \lambda$, then the validation loss is continuously differentiable with respect to $\boldsymbol \lambda$ at almost every $\boldsymbol \lambda$.

Suppose the theorem conditions are satisfied at $\boldsymbol \lambda_0$. Let $\boldsymbol B'$ be an orthonormal set of basis vectors that span the differentiable space $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$ with the subset of vectors $\boldsymbol B$ that span the model parameter space.

Let $\tilde L_T(\boldsymbol \theta,\boldsymbol \lambda)$ be the gradient of $L_T(\cdot, \boldsymbol \lambda)$ at $\boldsymbol \theta$ with respect to the basis $\boldsymbol B$:
\begin{equation}
\tilde L_T(\boldsymbol \theta,\boldsymbol \lambda) = _{\boldsymbol B}\nabla L_T(\cdot, \boldsymbol \lambda) |_{\boldsymbol \theta}
\end{equation}

Since $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$ is the minimizer of the training loss, the gradient of $L_T(\cdot, \boldsymbol \lambda_0)$ with respect to the basis $\boldsymbol B$ must be zero at $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$:
\begin{equation}
_{\boldsymbol B}\nabla L_T(\cdot, \boldsymbol \lambda_0)|_{\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)} = \tilde L_T(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0) = 0
\end{equation}

From our assumptions, we know that there exists a neighborhood $W$ containing $\boldsymbol \lambda_0$ such that $\tilde L_T$ is continuously differentiable along directions in the differentiable space $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$. Also, the Jacobian matrix $D \tilde L_T(\cdot, \boldsymbol \lambda_0)|_{\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)}$ with respect to basis $\boldsymbol B$ is nonsingular. Therefore, by the implicit function theorem, there exist open sets $U \subseteq W$ containing $\boldsymbol \lambda_0$ and $V$ containing $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$ and a continuously differentiable function $\gamma: U \rightarrow V$ such that for every $\boldsymbol \lambda \in U$, we have that 
\begin{equation}
\tilde L_T(\gamma(\boldsymbol \lambda), \boldsymbol \lambda) = \nabla_{B} L_T(\cdot, \boldsymbol \lambda)|_{\gamma(\boldsymbol \lambda)} = 0
\end{equation}
That is, we know that $\gamma(\boldsymbol \lambda)$ is a continuously differentiable function that minimizes $L_T(\cdot, \boldsymbol \lambda)$ in the differentiable space  $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$.
Since we assumed that the differentiable space is a local optimality space of $L_T(\cdot, \boldsymbol \lambda)$ in the neighborhood $W$, then for every $\boldsymbol \lambda \in U$, 
\begin{equation}
\hat {\boldsymbol \theta}(\boldsymbol \lambda) =
\argmin_{\boldsymbol \theta} L_T(\boldsymbol \theta, \boldsymbol \lambda) =
\argmin_{\boldsymbol \theta \in \Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)} L_T(\boldsymbol \theta, \boldsymbol \lambda) =
\gamma(\boldsymbol \lambda)
\end{equation}
Therefore, we have shown that if $\boldsymbol \lambda_0$ satisfies the assumptions given in the theorem, the fitted model parameters $\hat {\boldsymbol \theta}(\boldsymbol \lambda)$ is a continuously differentiable function within a neighborhood of $\boldsymbol \lambda_0$. We can then apply the chain rule to get the gradient of the validation loss.
\end{proof}

\subsection{Regression Examples}

\subsubsection{Elastic Net}\label{enet_conditions}
We show that the joint optimization problem for the Elastic Net satisfies all three conditions in Theorem~\ref{thethrm}:
\begin{itemize}
\item[] Condition 1: The elastic net solution paths are piecewise linear \citep{zou2003regression}, which means that the nonzero indices of the elastic net estimates stay locally constant for almost every $\boldsymbol{\lambda}$. Therefore, $S_{\boldsymbol{\lambda}}$ as defined in Section~\ref{sec:enet}  is a local optimality space for $L_T(\cdot, \boldsymbol{\lambda})$.
\item[] Condition 2: We only need to establish that the $\ell_1$ penalty is twice-continuously differentiable in the directions of $S_{\boldsymbol{\lambda}}$ since the quadratic loss function and the ridge penalty are both smooth. The absolute value function is twice-continuously differentiable everywhere except at zero. Hence the training criterion is smooth when restricted to $S_{\boldsymbol{\lambda}}$.
\item[] Condition 3: The Hessian matrix of $L_T(\cdot, \boldsymbol{\lambda})$ with respect to $\boldsymbol I_{I(\boldsymbol \lambda)}$ is $\boldsymbol I_{I(\boldsymbol \lambda)}^\top \boldsymbol{X}_{T}^\top \boldsymbol{X}_{T} \boldsymbol I_{I(\boldsymbol \lambda)} + \lambda_2 \boldsymbol{I}$. The first summand is positive semi-definite. As long as $\lambda_2 > 0$, the contribution of the identity matrix ensures the Hessian is positive definite.
\end{itemize}

\subsubsection{Additive Models with Sparsity and Smoothness Penalties}
\label{sec_appendix:sparse_add_models}
We use the notation in Section~\ref{sec:additive}. 
In addition, let  $|J(\boldsymbol{\lambda})|$ be the number of elements in $J(\boldsymbol{\lambda})$. Let
\begin{equation}
\boldsymbol{U} = \begin{bmatrix}
\boldsymbol {U}^{(i_1)} & ... & \boldsymbol {U}^{(i_{|J(\boldsymbol \lambda)|})}
\end{bmatrix}
\end{equation}
where $i_\ell \in J(\boldsymbol \lambda)$. 

The gradient of the validation loss is
\begin{equation}
\nabla_{\boldsymbol \lambda} L(\boldsymbol{y_V}, f_{\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}(\boldsymbol{X_V})) =
-
\left (
\boldsymbol{I}_V \sum_{i\in J(\boldsymbol\lambda)}  \boldsymbol {U}^{(i)} \frac{\partial}{\partial \boldsymbol\lambda} \hat{\boldsymbol{\beta}}^{(i)}(\boldsymbol{\lambda})
\right )^\top
\left (
\boldsymbol{y}_V - \boldsymbol{I}_V \sum_{i\in J(\boldsymbol\lambda)} \boldsymbol {U}^{(i)} \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol\lambda)
\right )
\end{equation}
where
\begin{equation}
\frac{\partial}{\partial \boldsymbol{\lambda}} 
\hat{\boldsymbol{\beta}}(\boldsymbol{\lambda})
= 
\boldsymbol{H}(\boldsymbol\lambda)^{-1}
\begin{bmatrix}
\boldsymbol{C}_0(\hat{\boldsymbol{\beta}}(\boldsymbol{\lambda}))
&
\boldsymbol C \left (\hat{\boldsymbol \beta}( \boldsymbol \lambda) \right )
\end{bmatrix}
\label{eq:additive_gradient}
\end{equation}

The Hessian $\boldsymbol{H}(\boldsymbol\lambda)$ is
\begin{equation}
\boldsymbol{H}(\boldsymbol\lambda)
= \boldsymbol{U}^\top \boldsymbol I_T^\top \boldsymbol I_T \boldsymbol{U}
+ \lambda_0 diag  \left ( \left \{
\frac{1}{||\boldsymbol {U}^{(i)}  \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)||_2} \left (
\boldsymbol I - \frac{\hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda) \hat{\boldsymbol{\beta}}^{(i)\top} (\boldsymbol \lambda)}{||\boldsymbol {U}^{(i)}  \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)||_2^2}
\right )
\right \}_{i \in J(\boldsymbol{\lambda})}
 \right )
+ \epsilon \boldsymbol I
\label{eq:add_hessian}
\end{equation}
The vector $\boldsymbol{C}_0(\hat{\boldsymbol{\beta}}(\boldsymbol{\lambda}))$ is a vertical stack of the vectors 
$$
\frac{\hat{\boldsymbol{\beta}}^{(i)}(\boldsymbol \lambda)}{\left \| \boldsymbol {U}^{(i)}  \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)\right \|_2}
$$
for $i \in J(\boldsymbol{\lambda})$.
The matrix $\boldsymbol C(\hat{\boldsymbol \beta}( \boldsymbol \lambda))$ has columns $i = 1,...,p$
\begin{equation}
\boldsymbol{C}_i(\hat{\boldsymbol \beta}( \boldsymbol \lambda))
= \begin{cases}
\begin{bmatrix}
\boldsymbol{0} \\
\boldsymbol {U}^{(i)\top}  \boldsymbol{D}^{(2)\top}_{\boldsymbol{x}_i} 
sgn \left ( \boldsymbol{D}^{(2)}_{\boldsymbol{x}_i} \boldsymbol {U}^{(i)} \hat{\boldsymbol{\beta}}^{(i)} ( \boldsymbol \lambda) \right ) \\
\boldsymbol{0} \\
\end{bmatrix}
& \text{ for } i \in J(\boldsymbol \lambda) \\
\boldsymbol{0}
& \text{ for } i \not\in J(\boldsymbol \lambda) \\
\end{cases}
\end{equation}

Now we check that all three conditions are satisfied. 
\begin{itemize}
	\item[] Condition 1: It seems likely that the space spanned by $S_{\boldsymbol{\lambda}}$ is a local optimality space, though we are unable to formally prove this. The training criterion for this problem is composed of generalized lasso penalties and a group lasso penalties. For the generalized lasso, \citet{tibshirani2011solution} proved that the solution path is smooth almost everywhere. For the group lasso, there is empirical evidence that the active set is locally constant almost everywhere with respect to the penalty parameter \citep{yuan2006model}, but this has not been formally proven. \citet{vaiter2012degrees} showed that the active set is locally constant with respect to the response; we suspect similar techniques could be used to prove our hypothesis.
	% However, the dual formulation of this problem suggests that this condition holds. In particular, if we create dummy constraints $\boldsymbol{z}^{(i)} = \boldsymbol{I}_T\boldsymbol{\theta}^{(i)}$ and $\boldsymbol{w}^{(i)} = \boldsymbol{D}_{x_i}^{(2)} \boldsymbol{\theta}^{(i)}$ with corresponding dual variables $\boldsymbol{u}^{(i)}$ and $\boldsymbol{v}^{(i)}$, we find that the dual variables must satisfy constraints of the form $\|\boldsymbol{I}_T^\top \boldsymbol{u}^{(i)}\|_2 \le \lambda_0$ and $\|\boldsymbol{v}^{(i)}\|_\infty \le \lambda_i$. It therefore seems likely that the dual solution perturbs smoothly with $\boldsymbol{\lambda}$. Under some regularity conditions, there is also a smooth mapping between the primal and dual solutions. Hence the fitted model probably smooth with respect to $\boldsymbol{\lambda}$. Finally, assuming that the space spanned by the nonzero coefficients of $\hat{\boldsymbol{\theta}}\boldsymbol{\lambda}$ is a local optimality space, then clearly the local optimality space is also a differentiable space.
	\item[] Condition 2:  We only need to establish that the generalized lasso and group lasso penalties are twice-continuously differentiable in the directions of $S_{\boldsymbol{\lambda}}$ since the rest of the training criterion is smooth. 
	$\| \boldsymbol{D} \boldsymbol{\theta} \|_1$ is not differentiable at the points where $\boldsymbol{D} \boldsymbol{\theta}$ has zero elements. We must therefore restrict the derivatives to be taken in directions such that the zero elements of $\boldsymbol{D} \boldsymbol{\theta}$ remain constant. The $\ell_2$ norm is twice-continuously differentiable everywhere except at the zero vector. Hence the training criterion is smooth when restricted to the differentiable space $S_{\boldsymbol{\lambda}}$ specified in Section~\ref{sec:additive}.
	\item[] Condition 3: The Hessian matrix in \eqref{eq:add_hessian} is the sum of positive semi-definite matrices. As long as $\epsilon > 0$, the contribution of the last summand $\epsilon \boldsymbol{I}$ will make the Hessian matrix positive-definite.
\end{itemize}

\subsubsection{Un-pooled Sparse Group Lasso}

The gradient of the validation loss with respect to the penalty parameters is
\begin{equation}
\nabla_{\boldsymbol \lambda} L(\boldsymbol{y_V}, f_{\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}(\boldsymbol{X_V})) =
-\left (
\boldsymbol{X}_{V, I(\boldsymbol\lambda)}
\frac{\partial}{\partial \boldsymbol\lambda} \hat{\boldsymbol{\beta}}(\boldsymbol{\lambda})
\right )^\top
\left (
\boldsymbol{y}_V - \boldsymbol{X}_{V, I(\boldsymbol\lambda)} \hat{\boldsymbol{\beta}}(\boldsymbol{\lambda})
\right )
\end{equation}
where 
\begin{equation}
\frac{\partial}{\partial \boldsymbol \lambda} \hat{\boldsymbol{\beta}}(\boldsymbol{\lambda})
=
- \boldsymbol H(\boldsymbol \lambda)^{-1}
\begin{bmatrix}
\boldsymbol C(\hat{\boldsymbol \beta}(\boldsymbol \lambda)) & sgn(\hat {\boldsymbol \beta}(\boldsymbol \lambda))
\end{bmatrix}
\label{eq:unpooled_sgl_grad}
\end{equation}
The Hessian $\boldsymbol H(\boldsymbol \lambda)$ is
\begin{equation}
\boldsymbol{H}(\boldsymbol\lambda) =
\frac{1}{n} \boldsymbol X_{T, I(\boldsymbol \lambda)}^\top \boldsymbol X_{T, I(\boldsymbol \lambda)}
+ diag\left(
\frac{\lambda_m}{|| \boldsymbol \theta^{(m)}||_2}
\left (
\boldsymbol I - 
\frac{\boldsymbol \theta^{(m)} \boldsymbol \theta^{(m) \top}}{|| \boldsymbol \theta^{(m)}||_2^2}
\right )
\right)
+ \epsilon \boldsymbol I
\label{eq:sgl_hessian}
\end{equation}
The matrix $\boldsymbol C(\hat {\boldsymbol \beta}(\boldsymbol \lambda))$ in \eqref{eq:unpooled_sgl_grad} has columns $m=1,2...,M$ 
\begin{equation}
\boldsymbol{C}_i(\hat{\boldsymbol \beta}( \boldsymbol \lambda))
=
\begin{bmatrix}
\boldsymbol 0\\
\frac{\hat {\boldsymbol \beta}^{(m)}(\boldsymbol \lambda)}{||\hat{\boldsymbol \beta}^{(m)}(\boldsymbol \lambda)||_2}\\
\boldsymbol 0\\
\end{bmatrix}
\end{equation}
where $\boldsymbol 0$ are the appropriate dimensions.

The logic for checking all three conditions in Theorem~\ref{thethrm} is similar to the other examples:
\begin{itemize}
	\item[] Condition 1: We hypothesize that the differentiable space $S_{\boldsymbol{\lambda}}$ is also a local optimality space, though we have not formally proven this fact. We suspect this is true for the same reasons discussed in Section \ref{sec_appendix:sparse_add_models}.
	%it is difficult to formally prove that the local optimality space at $\boldsymbol{\lambda}$ is the space spanned by the nonzero coefficients of $\boldsymbol{\theta}$. However if we follow very similar reasoning, the dual formulation of this problem suggests that the fitted model perturbs smoothly in $\boldsymbol{\lambda}$ almost everywhere. Hence it seems likely that the set of nonzero coefficients $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$ is constant at almost every $\boldsymbol{\lambda}$. Assuming this is true, then the local optimality space $S_{\boldsymbol{\lambda}}$ is clearly also a differentiable space.
	\item[] Condition 2: The $\ell_1$ and $\ell_2$ penalties are twice-differentiable when restricted to $S_{\boldsymbol{\lambda}}$ for the same reasons discussed in Section~\ref{sec_appendix:sparse_add_models}. 
	\item[] Condition 3: The Hessian matrix in \eqref{eq:sgl_hessian} is the sum of positive semi-definite matrices. It is positive definite for any $\epsilon > 0$ due to the last summand $\epsilon \boldsymbol{I}$. 
\end{itemize}

\subsubsection{Low-rank Matrix Completion}
Here we derive the differentiable space of the training criterion with respect to $\boldsymbol{\Gamma}$. At $\boldsymbol{\lambda}$, suppose the fitted interaction matrix $\hat{\boldsymbol{\Gamma}}(\boldsymbol{\lambda})$ has a singular value decomposition $\hat{\boldsymbol{U}}(\boldsymbol{\lambda}) \text{diag}(\hat{\boldsymbol{\sigma}}(\boldsymbol{\lambda})) \hat{\boldsymbol{V}}^\top(\boldsymbol{\lambda})$. We denote the $i$th singular value/vector with subscript $i$. Then the differentiable  space with respect to $\boldsymbol{\Gamma}$ at $\hat{\boldsymbol{\Gamma}}(\boldsymbol{\lambda})$ is
\begin{align}
S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}} & = 
\left\{
\boldsymbol{B} \in \mathbb{R}^{N \times N}
\middle |
\hat{\boldsymbol{U}}_i^\top(\boldsymbol{\lambda}) \boldsymbol{B} \hat{\boldsymbol{V}}_i(\boldsymbol{\lambda}) = 0 \quad \forall i \text{ s.t. } \sigma_i  = 0
\right\}
\\
& = 
\text{span} \left (
\left\{
\hat{\boldsymbol{U}}_i(\boldsymbol{\lambda}) \boldsymbol{b}_u^\top + \boldsymbol{b}_v \hat{\boldsymbol{V}}_i^\top(\boldsymbol{\lambda})
\middle |
\boldsymbol{b}_u, \boldsymbol{b}_v \in \mathbb{R}^{N},
\sigma_i \ne 0
\right\}
\right )
\label{eq:diff_space_gamma}
\end{align}
The proof is a direct application of Theorem 1 in \citet{watson1992characterization}. The following lemma adapts his results for our purposes. Note that if a matrix can be written as a univariate function $\tilde{\boldsymbol{\Gamma}}(\epsilon)$, its singular values and singular vectors can be numbered such that they are each a function of $\epsilon$, e.g. $\sigma_i(\epsilon)$, $\boldsymbol{U}_i(\epsilon)$, and $\boldsymbol{V}_i(\epsilon)$ \citep{rellich1969perturbation}.
\begin{lemma}
	Suppose $\boldsymbol{\Gamma} \in \mathbb{R}^{N\times N}$ has a singular value decomposition $\boldsymbol{U}\text{diag}(\boldsymbol{\sigma})\boldsymbol{V}$.
	Let 
	\begin{align}
	\mathcal{B} & = 
	\left\{
	\boldsymbol{B} \in \mathbb{R}^{N \times N}
	\middle |
	{\boldsymbol{U}}_i^\top \boldsymbol{B} {\boldsymbol{V}}_i = 0 \quad \forall i \text{ s.t. } \sigma_i  = 0
	\right\}
	\end{align}
	The directional derivative of the nuclear norm $\| \cdot \|_*$ at $\boldsymbol{\Gamma}$ along $\boldsymbol{B} \in \mathcal{B}$ is
	\begin{equation}
	\lim_{\epsilon\rightarrow 0^+} \frac{\| \boldsymbol{\Gamma} + \epsilon\boldsymbol{B} \|_* - \| \boldsymbol{\Gamma} \|_*}{\epsilon}
	=
	\sum_{i=1}^N \boldsymbol{U}_i^\top \boldsymbol{B} \boldsymbol{V}_i 1_{[\sigma_i \ne 0]}
	\label{eq:direct_deriv_nuclear_norm}
	\end{equation}
	
	Moreover, let the eigenvalues be numbered such that $\sigma_{i, \boldsymbol{B}}(\epsilon)$ denotes the $i$th singular value of $\boldsymbol{\Gamma} + \epsilon \boldsymbol{B}$. Then
	\begin{equation}
	\mathcal{B} = \left \{
	\boldsymbol{B} \in \mathbb{R}^{N \times N}
	\middle |
	\left . \frac{d \sigma_{i, \boldsymbol{B}} (\epsilon )}{d\epsilon} \right |_{\epsilon = 0} = 0  \quad\forall i \text{ s.t. } \sigma_i= 0
	\right \}
	\label{eq:const_singular_values}
	\end{equation}
	
\end{lemma}

Now we derive the gradient of the validation loss with respect to the penalty parameters. One approach would be to follow Algorithm~\ref{alg:gradDescent} exactly, which requires us to find an orthonormal basis of \eqref{eq:diff_space_gamma}. 
An alternative approach is to use the result in \eqref{eq:const_singular_values}: the differentiable space is the set of directions where the zero singular values remain locally constant. Assuming Condition~\ref{condn:local_is_diff} holds, we only need to consider interaction matrices with rank at most $r = \text{rank}(\hat{\boldsymbol{\Gamma}}(\boldsymbol{\lambda}))$. Hence a locally equivalent training criterion is:
\begin{align}
\begin{split}
\argmin_{
	\substack{%
		\boldsymbol{\eta}, \boldsymbol{\gamma} \\
		\boldsymbol{\Gamma} = \boldsymbol{U}diag(\boldsymbol{\sigma}) \boldsymbol{V}^\top\\
		\boldsymbol{U}, \boldsymbol{V} \in \mathbb{R}^{N\times r},
		\boldsymbol{\sigma} \in \mathbb{R}^{r}
	}
}
& 
\frac{1}{2} 
\left \| 
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \boldsymbol{\eta} \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \boldsymbol{\gamma} \boldsymbol{1}^\top )^\top
- \boldsymbol{\Gamma}
\right \|^2_T
+ \lambda_0  \left \| \boldsymbol{\Gamma} \right  \|_* \\
& + \sum_{g\in J_{\boldsymbol{\alpha}}(\boldsymbol{\lambda})}  \lambda_g \| \boldsymbol\eta^{(g)} \|_2
+ \sum_{g\in J_{\boldsymbol{\beta}}(\boldsymbol{\lambda})}  \lambda_{G+g} \| \boldsymbol\gamma^{(g)} \|_2
+ \frac{1}{2} \epsilon \left (
\| \boldsymbol\eta \|_2^2 + \| \boldsymbol\gamma \|_2^2 
+ \left  \| \boldsymbol{\Gamma} \right \|^2_F
\right )
\label{eq:matrix_comp_groups_svd_smooth}
\end{split}
\\
& 
\text{s.t. } 
\boldsymbol{V}^\top \boldsymbol{V} = \boldsymbol{I}
\text{ and } \boldsymbol{U}^\top \boldsymbol{U} = \boldsymbol{I}
\label{eq:orthonormal_constraints}
\end{align}
The locally equivalent training criterion is now smooth at its minimizer.
The gradient optimality conditions with respect to $\boldsymbol{\Gamma}$ can be taken with respect to the basis 
\begin{align}
\left \{
\hat{\boldsymbol{U}}_i(\boldsymbol{\lambda}) \boldsymbol{e}_j^\top  | i = 1,...,r; j = 1,...,N
\right \}
\cup
\left \{
\boldsymbol{e}_j \hat{\boldsymbol{V}}_i(\boldsymbol{\lambda})^\top | i = 1,...,r; j = 1,...,N
\right \}
\label{eq:mat_completion_gamma_basis}
\end{align}
Note that this basis is quite different from that used in Algorithm \ref{alg:gradDescent}; it is allowed to vary with $\boldsymbol{\lambda}$ and its elements are not orthonormal. The benefit of this alternative approach is that the gradient optimality condition for $\boldsymbol{\Gamma}$ is easy to derive. Taking the gradient with respect to the directions in \ref{eq:mat_completion_gamma_basis}, we get:
\begin{align}
\begin{split}
\boldsymbol{0} & = 
- \hat{\boldsymbol{U}}(\boldsymbol{\lambda})^\top
\left (
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  \boldsymbol{1}^\top )^\top
- \hat{\boldsymbol{U}}(\boldsymbol{\lambda})\text{diag}(\hat{\boldsymbol{\sigma}}(\boldsymbol{\lambda})) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
\right )_T
\\
& \qquad + \lambda_0 \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
+ \epsilon \text{diag}(\hat{\boldsymbol{\sigma}}(\boldsymbol{\lambda})) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
\end{split}
\label{eq:grad_opt_matrix_left}
\\
\begin{split}
\boldsymbol{0} & = -\left (
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  \boldsymbol{1}^\top )^\top
- \hat{\boldsymbol{U}}(\boldsymbol{\lambda})\text{diag}(\hat{\boldsymbol{\sigma}}(\boldsymbol{\lambda})) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
\right )_T
\hat{\boldsymbol{V}}(\boldsymbol{\lambda})
\\
& \qquad + \lambda_0 \hat{\boldsymbol{U}}(\boldsymbol{\lambda})
+ \epsilon \hat{\boldsymbol{U}}(\boldsymbol{\lambda}) \text{diag}(\hat{\boldsymbol{\sigma}}(\boldsymbol{\lambda}))
\end{split}
\label{eq:grad_opt_matrix_comp}
\end{align}
where $(\cdot)_T$ zeroes out matrix elements that are not observed in the training set. The gradient optimality conditions with respect to $\boldsymbol{\eta}$ and $\boldsymbol{\gamma}$ are derived using the usual procedure. To get the partial derivatives of the fitted values with respect to $\boldsymbol{\lambda}$, we implicitly differentiate the gradient optimality conditions, as well as \eqref{eq:orthonormal_constraints}, with respect to $\boldsymbol{\lambda}$ and solve the resulting system of linear equations. The gradient of the validation loss with respect to the penalty parameters is straightforward to calculate once the partial derivatives are obtained. However, we omit this tedious calculation.

We now show that the conditions in Theorem~\ref{thethrm} are satisfied.
\begin{itemize}
	\item[] Condition 1: We hypothesize that the differentiable space $S_{\boldsymbol{\lambda}}$ defined in \eqref{eq:matrix_completion_diff_space} is also a local optimality space $\boldsymbol{\lambda}$. For the group lasso penalties, we use the same reasons mentioned in \ref{sec_appendix:sparse_add_models} to justify this hypothesis. For the nuclear norm penalty, it has been observed empirically that small perturbations in the penalty parameter result in matrices with similar rank \citep{mazumder2010spectral}. This supports our belief that $S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}}$ is a local optimality space with respect to $\boldsymbol{\Gamma}$ at $\boldsymbol{\lambda}$.
	\item[] Condition 2: The only non-smooth components of the training criterion are the group lasso and nuclear norm penalties. The group lasso penalty is twice-differentiable when restricted to the differentiable space, using the same reasoning in Section~\ref{sec_appendix:sparse_add_models}. From \eqref{eq:direct_deriv_nuclear_norm}, we see that the nuclear norm $\|\boldsymbol{\Gamma}\|_{*}$ is also twice-differentiable with respect to $\boldsymbol{\Gamma}$ when restricted to $S_{\boldsymbol{\lambda}, \boldsymbol{\Gamma}}$.

	\item[] Condition 3: The differentiable space for the training criterion with respect to $\boldsymbol{\Gamma}$ is a linear space. Therefore there exists some orthonormal basis of the differentiable space. Since the training criterion is the sum of convex functions with ridge penalties on all the variables, the Hessian of the training criterion is positive definite for any $\epsilon > 0$.
	
\end{itemize}

\subsection{Gradient Descent Details}\label{sec:alg_details}
Here we discuss our choice of step size and convergence criterion in gradient descent.

There are many possible choices for our step size sequence $\{t^{(k)}\}$ \citep{boyd2004convex}. We chose a backtracking line, which we describe here briefly. Let the criterion function be $L:\mathbb{R}^n \rightarrow \mathbb{R}$. Suppose that the descent algorithm is currently at point $x$ with descent direction $\Delta x$. The algorithm is given below. It depends on constants $\alpha  \in (0, 0.5)$ and $\beta \in (0,1)$.
\begin{algorithm}
	\caption{Backtracking Line Search} \label{alg:backtracking}
	\begin{algorithmic}
		\STATE{Initialize $t= 1$.} \\
		\WHILE{$L(\boldsymbol x + t \boldsymbol \Delta \boldsymbol x) > L(\boldsymbol x) + \alpha t \nabla L(\boldsymbol x)^T \boldsymbol \Delta \boldsymbol x$}
		\STATE{Update $t := \beta t$}
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}
In our examples initial step size was 1, and we backtrack with parameters $\alpha = 0.001$ and $\beta = 0.1$. During gradient descent, it is possible that the step size will result in a negative regularization parameter; we reject any step that would set a regularization parameter to below a minimum threshold of $1e$-6.

Our convergence criterion is based on the change in our validation loss between iterates. More specifically, we stop our algorithm when
\[
L \left( \boldsymbol{y}_V, f_{\hat{\boldsymbol \theta}(\boldsymbol{\lambda}^{(k+1)})}(\boldsymbol{X}_V)\right) -
L \left( \boldsymbol{y}_V, f_{\hat{\boldsymbol \theta}(\boldsymbol{\lambda}^{(k)})}(\boldsymbol{X}_V)\right) \leq \delta
\]
For the results in this manuscript we use $\delta = 0.0005$.

\subsection{Sensitivity to initialization points}
Since the results of gradient descent and Nelder-Mead depend on their initialization points, we ran a simulation to see how sensitive the methods were to where they were initialized and how many initializations were used.

We tested a smaller version of the joint optimization problem in  Section~\ref{sec:additive}. Here we use 60 training, 30 validation, and 30 test observations and $p = 15$ covariates. The response was generated from \eqref{eq:simulation_sparse_add}. We initialized $\boldsymbol{\lambda}$ by considering all possible combinations of $(\lambda_0, \lambda_1 \boldsymbol{1})$ where $\lambda_0, \lambda_1 \in \{10^i: i\in\{-2, -1, 0, 1\}\}$.

In Figure \ref{fig:mult_starts} (left), we plot the validation error as the number of initializations increases. The validation errors from both methods plateau quickly. Gradient descent manages to find penalty parameters with lower validation error than Nelder-Mead. Figure \ref{fig:mult_starts} (right) presents the distribution of validation errors resulting from the random initializations. On average, gradient descent finds penalty parameters with lower validation error compared to Nelder-Mead. The plots show that the methods are indeed sensitive to their initialization points. For example, one could run a very coarse grid search on the two-parameter version of the joint optimization problem and use the best penalty parameter values.

\begin{figure}
	\caption{\label{fig:mult_starts}
		Error of additive models tuned by Gradient Descent vs. Nelder-Mead. Left: Validation error of models after as the number of initialization points increases. Right: The distribution of validation errors. (Gradient Descent = GD, Nelder-Mead = NM)
	}
	\centering
	\includegraphics[width=0.45\textwidth]{many_inits_3_12_60_30_30_2_16_trend.png}
	\includegraphics[width=0.45\textwidth]{many_inits_3_12_60_30_30_2_16_box.png}
\end{figure}

\subsection{Additional simulation results}

The simulation results in Section~\ref{sec:results} show that joint optimization problems with many penalty parameters can produce better models than those with only two penalty parameters. One may wonder if this difference is due to the method used to tune the penalty parameters. Here we present results from tuning the two-penalty-parameter joint optimization problems from Sections~\ref{sec:simulation_sparse_add}, \ref{sec:simulation_sgl}, and \ref{sec:simulation_matrix} using gradient descent, Nelder-Mead, and Spearmint. As shown in Table~\ref{tab:two_params}, the performance of these methods are very similar to grid search. Regardless of the method used to tune the two-penalty parameter joint optimization, the resulting models all have higher validation and test error compared to the models from the joint optimization problem with many penalty parameters tuned by gradient descent.

\begin{table}
	\caption {\label{tab:two_params} Two-parameter joint optimization problems for the examples in Section~\ref{sec:results}. Standard errors are given in parentheses. We abbreviated the methods as follows: Gradient Descent = GD, Nelder-Mead = NM, Spearmint = SP, Grid Search = GS}
	\centering
	\begin{tabular}{| l | l | l | l | }
	\hline
	\multicolumn{4}{|c|}{Sparse additive models}\\
	\hline
	& Validation Error & Test Error & \# Solves\\
	\hline
	% bayes: script_out/sparse_hc0_new_numsolves.out
	GD & 23.87 (0.97) & 26.10 (0.86) & 13.07 \\
	\hline
	% bayes: script_out/sparse_nm0.out
	NM & 28.86 (1.04) & 29.97 (0.96) & 100 \\
	\hline
	% bayes: script_out/sparse_sp0.out
	SP & 29.18 (1.07) & 30.09 (1.08) & 100 \\
	\hline
	% see other file
	GS & 28.71 (0.97) & 29.42 (0.96) & 100 \\
	\hline
	\end{tabular}

	\vspace{0.5cm}

	\begin{tabular}{| l | l | l | l | }
		\hline
		\multicolumn{4}{|c|}{Sparse Group Lasso}\\
		\hline
		\multicolumn{4}{|c|}{n=90, p=600, M=30}\\
		\hline
		& Validation Err & Test Err & \# Solves \\
		\hline
		% bayes: script_out/sgl_hc0_1.txt
		GD & 46.82 (2.21) & 49.33 (1.36)& 21.43\\
		\hline
		% bayes: script_out/sgl_nm0_1_90_30_p600.out
		NM & 46.37 (2.24) & 48.95 (1.35) & 100 \\
		\hline
		% bayes: script_out/sgl_sp0_1.out
		SP &  45.70 (2.32) & 49.35 (1.56) & 100 \\
		\hline
		% see other file
		GS & 47.23 (2.26) & 50.01 (1.40) & 100 \\
		\hline
		\multicolumn{4}{|c|}{n=90, p=900, M=60}\\
		\hline
		 & Validation Error & Test Error & \# Solves \\
		\hline
		% bayes: script_out/sgl_hc0_2.out
		GD  & 45.71 (2.26) & 50.31 (1.93) & 20.77\\
		\hline
		% bayes: script_out/sgl_nm0_2.out
		NM  & 44.95 (2.24) & 50.18 (1.82) & 100  \\
		\hline
		% bayes: script_out/sgl_sp0_2.out
		SP  & 49.59 (2.27) & 56.54 (2.14) & 100 \\
		\hline
		% see other file
		GS & 45.70 (2.27) & 51.34 (1.86) & 100 \\
		\hline
		\multicolumn{4}{|c|}{n=90, p=1200, M=100}\\
		\hline
		&  Validation Error & Test Error & \# Solves \\
		\hline
		% bayes: script_out/sgl_hc0_3.out
		GD & 50.46 (2.30) & 57.02 (1.94) & 19.80 \\
		\hline
		% bayes: script_out/sgl_nm0_3.out
		NM & 49.92 (2.33) & 55.46 (1.89) & 100 \\
		\hline
		% bayes: script_out/sgl_sp0_3.out
		SP  & 49.70 (2.26) & 56.51 (2.16) & 100 \\
		\hline
		% see other file
		GS & 50.00 (2.16) & 57.14 (2.18) & 100 \\
		\hline
	\end{tabular}
	
	\vspace{0.5cm}
	
	\begin{tabular}{| l | l | l | l |}
		\hline
		\multicolumn{4}{|c|}{Low-rank Matrix Completion}\\
		\hline
		& Validation Err & Test Err &  Num Solves\\
		\hline
		% bayes: script_out/matrix_completion_groups_HC0_11_new2.txt
		GD  & 0.70 (0.04) &  0.71 (0.04) & 8.03 (0.79) \\
		\hline
		% bayes: script_out/matrix_completion_groups_NM0_11_new1.txt
		NM & 0.71 (0.04) & 0.71 (0.04) & 100 \\
		\hline
		% bayes: script_out/matrix_completion_groups_SP0_11_new.txt
		SP & 0.73 (0.04) & 0.74 (0.04) & 100\\
		\hline
		% see other file
		GS & 0.71 (0.04) & 0.72 (0.04) & 100\\
		\hline
	\end{tabular}
	
\end{table}

\subsection{Smoothness of the Validation Loss}

Since our algorithm depends on the validation loss being smooth almost everywhere, a potential concern is that the validation loss may not be differentiable at the solution of the joint optimization problem. We address this concern empirically. Based on the simulation study below, we suspect that the minimizer falls exactly at a knot (where our validation loss is not differentiable with respect to $\boldsymbol{\lambda}$) with measure zero. 

In this simulation we solved a penalized least squares problem with a lasso penalty and tuned the penalty parameter to minimize the loss on a separate validation set. We considered a linear model with 100 covariates. The training and validation sets included 40 and 30 observations, respectively. The response was generated data from the model
$$
y = \boldsymbol{X}\boldsymbol{\beta} + \sigma\epsilon
$$
where $\boldsymbol{\beta} = (1, 1, 1, 0, ..., 0)$. $\epsilon$ and $\boldsymbol{X}$ were drawn independently from a standard Gaussian distribution. $\sigma$ was chosen so that the signal to noise ratio was 2. For a given $\lambda>0$ our fitted $\boldsymbol{\beta}$ minimized the penalized training criterion
$$
\boldsymbol{\hat{\beta}}(\lambda) = \arg\min_{\boldsymbol{\beta}} \| \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} \|_T^2 + \lambda \|\boldsymbol{\beta}\|_1
$$
We then chose the $\lambda$-value for which $\boldsymbol{\hat{\beta}}(\lambda)$ minimized the validation error.

In our 500 simulation runs, the penalty parameter that minimized the validation loss was never located at a knot: Using a homotopy solver for the lasso, we were able to find the \emph{exact} knots ($\lambda$-values where variables enter/leave the model), and these points never achieved the minimum value of the validation loss. While this is only one example, and not definitive proof, we believe it is a strong indication that it is unlikely for solutions to occur regularly at knots in penalized problems.

In addition, we believe that the behavior of our procedure is analogous to solving the Lasso via sub-gradient descent. In the Lasso setting, sub-gradient descent with a properly chosen step-size will converge to the solution. In addition, if initialized at a differentiable $\beta$-value (ie. with all non-zero entries), then the lasso objective will be differentiable at all iterates in this procedure with probability one. Admittedly, using the sub-gradient method to solve the lasso has fallen out of favor. The current gold-standard methods, such as generalized gradient descent, give sparse solutions at large enough iterates and achieve faster convergence rates.



%\subsubsection{Sparse add models}
%
%\begin{table}
%	\caption {\label{tab:additive} Additive models fitted with two penalty parameters, tuned by gradient descent, Nelder-Mead, and Spearmint. Standard errors are given in parentheses.}
%	\centering
%	\begin{tabular}{| l | l | l | l | l | }
%		\hline
%		& Num $\lambda$ & Validation Error & Test Error & \# Solves\\
%		\hline
%		% bayes: script_out/sparse_hc0_new_numsolves.out
%		Gradient Descent & 2 & 23.87 (0.97) & 26.10 (0.86) & 13.07 \\
%		\hline
%		% bayes: script_out/sparse_nm0.out
%		Nelder-Mead & 2 & 28.86 (1.04) & 29.97 (0.96) & 100 \\
%		\hline
%		% bayes: script_out/sparse_sp0.out
%		Spearmint & 2 & 29.18 (1.07) & 30.09 (1.08) & 100 \\
%		\hline
%	\end{tabular}
%\end{table}


%\subsubsection{Un-pooled Sparse group lasso}
%Table \ref{table:two_param_sgl} displays results from fitting the two-parameter version of the joint optimization problem \eqref{eq:unpooled_sgl} using gradient descent, Nelder-Mead, and Spearmint. Comparing the results in \label{table:unpooled}, we see that all four methods give similar the validation and test errors when tuning the model with two penalty parameters. Therefore regardless of the method used to tune the two-parameter sparse group lasso, the un-pooled sparse group lasso gives models with significantly lower test error \textcolor{red}{I think you need to say something more here; this seems like the point is a referee response, but no one else reading the appendix knows what the referee reports say!}.
%\begin{table}
%\caption{\label{table:two_param_sgl} Sparse Group Lasso fitted with two penalty parameters. Standard errors are given in parentheses. We abbreviated the methods as follows: Gradient Descent = GD, Nelder-Mead = NM, Spearmint = SP}
%\centering
%\begin{tabular}{| l | l | l | l | l | l | }
%	\hline
%	\multicolumn{6}{|c|}{n=90, p=600, M=30}\\
%	\hline
%	& \# $\lambda$ & $\beta$ Error & Validation Err & Test Err & \# Solves \\
%	\hline
%	% bayes: script_out/sgl_hc0_1.txt
%	GD & 2 & 7.37 (0.18) & 46.82 (2.21) & 49.33 (1.36)& 21.43\\
%	\hline
%	% bayes: script_out/sgl_nm0_1_90_30_p600.out
%	NM & 2 & 7.31 (0.18)  & 46.37 (2.24) & 48.95 (1.35) & 100 \\
%	\hline
%	% bayes: script_out/sgl_sp0_1.out
%	SP & 2 & 7.35 (0.20) &  45.70 (2.32) & 49.35 (1.56) & 100 \\
%	\hline
%	\multicolumn{6}{|c|}{n=90, p=900, M=60}\\
%	\hline
%	& \# $\lambda$ & $\beta$ Error & Validation Error & Test Error & \# Solves \\
%	\hline
%	% bayes: script_out/sgl_hc0_2.out
%	GD & 2 & 7.58 (0.21) & 45.71 (2.26) & 50.31 (1.93) & 20.77\\
%	\hline
%	% bayes: script_out/sgl_nm0_2.out
%	NM & 2 & 7.56 (0.19) & 44.95 (2.24) & 50.18 (1.82) & 100  \\
%	\hline
%	% bayes: script_out/sgl_sp0_2.out
%	SP & 2 & 8.20 (0.20)  & 49.59 (2.27) & 56.54 (2.14) & 100 \\
%	\hline
%	\multicolumn{6}{|c|}{n=90, p=1200, M=100}\\
%	\hline
%	& \# $\lambda$ & $\beta$ Error & Validation Error & Test Error & \# Solves \\
%	\hline
%	% bayes: script_out/sgl_hc0_3.out
%	GD & 2 & 8.27 (0.19) & 50.46 (2.30) & 57.02 (1.94) & 19.80 \\
%	\hline
%	% bayes: script_out/sgl_nm0_3.out
%	NM & 2 & 8.09 (0.19) & 49.92 (2.33) & 55.46 (1.89) & 100 \\
%	\hline
%	% bayes: script_out/sgl_sp0_3.out
%	SP & 2 & 8.20 (0.20) & 49.70 (2.26) & 56.51 (2.16) & 100 \\
%	\hline
%\end{tabular}
%\end{table}
%
%\subsubsection{Low-rank Matrix Completion}
%\textcolor{red}{I'm confused on what this section is trying to get across}. The results from fitting the two-parameter version of the joint optimization problem \eqref{eq:matrix_comp_groups} using gradient descent, Nelder-Mead, and Spearmint are displayed in Table \ref{table:two_param_matrix_completion}. Comparing these results to those in Table \label{table:matrix_completion} \textcolor{red}{I think this citation is having an issue?}, we see that these methods produce similar validation and test errors as grid search. More importantly, these results show that having separate penalty parameters for each of the covariate groups results in lower test error, regardless of the method used to tune the two-parameter joint optimization problem \textcolor{red}{(I thought in the last sentence you were saying they were the same as grid search?)}.
%
%\begin{table}
%	\caption{\label{table:two_param_matrix_completion} Matrix Completion fitted with two penalty parameters. Standard errors are given in parentheses. We abbreviated the methods as follows: Gradient Descent = GD, Nelder-Mead = NM, Spearmint = SP}
%	\centering
%	\begin{tabular}{| l | l | l | l | l |}
%		\hline
%		& \# $\lambda$ & Validation Err & Test Err &  Num Solves\\
%		\hline
%		% bayes: script_out/matrix_completion_groups_HC0_11_new2.txt
%		GD & 2 & 0.70 (0.04) &  0.71 (0.04) & 8.03 (0.79) \\
%		\hline
%		% bayes: script_out/matrix_completion_groups_NM0_11_new1.txt
%		NM & 2 & 0.71 (0.04) & 0.71 (0.04) & 100 \\
%		\hline
%		% bayes: script_out/matrix_completion_groups_SP0_11_new.txt
%		SP & 2 & 0.73 (0.04) & 0.74 (0.04) & 100\\
%		\hline
%	\end{tabular}
%\end{table}

\bibliographystyle{agsm}
\bibliography{hillclimbing_nonsmooth_appendix}

\end{document}
