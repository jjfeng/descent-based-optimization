\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}


% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

%%%% Packages and definitions
\usepackage{xr}
\externaldocument{hillclimbing_nonsmooth}

\usepackage[top=0.85in,left=1.0in,right=1.0in,footskip=0.75in]{geometry}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

\usepackage{algorithm,algorithmic}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

\usepackage{amsthm}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% for the beautiful checkmarks
\usepackage{pifont}

\DeclareMathOperator*{\argmin}{arg\,min}
%%%%

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\thealgorithm}{}

\section{Appendix}

\subsection{$K$-fold Cross Validation}
We can perform joint optimization for $K$-fold cross validation by reformulating the problem. Let $(\boldsymbol y, \boldsymbol{X})$ be the full data set. We denote the $k$th fold as $(\boldsymbol y_{k}, \boldsymbol{X}_{k})$ and its complement as $(\boldsymbol y_{-k}, \boldsymbol{X}_{-k})$. Then the objective of this joint optimization problem is the average validation cost across all $K$ folds:
\begin{equation}
\begin{array}{c}
\argmin_{\boldsymbol{\lambda} \in \Lambda} \frac{1}{K} \sum_{k=1}^K L(\boldsymbol{y}_{k}, f_{\hat{\boldsymbol \theta}^{(k)}(\boldsymbol{\lambda})}(\boldsymbol{X}_k)) \\
\text{s.t. } {\hat{\boldsymbol \theta}^{(k)}(\boldsymbol{\lambda})} = \argmin_{\boldsymbol \theta \in \Theta} L(\boldsymbol{y}_{-k}, f_{\boldsymbol \theta} (\boldsymbol{X}_{-k})) + \sum\limits_{i=1}^J \lambda_i P_i(\boldsymbol \theta) \text{ for } k=1,...,K
\end{array}
\label{jointoptFullCV}
\end{equation}

\subsection{Proof of Theorem~\ref{thethrm}}

\begin{proof}
We will show that for a given $\boldsymbol \lambda_0$ that satisfies the given conditions, the validation loss is continuously differentiable within some neighborhood of $\boldsymbol \lambda_0$.  It then follows that if the theorem conditions hold true for almost every $\boldsymbol \lambda$, then the validation loss is continuously differentiable with respect to $\boldsymbol \lambda$ at almost every $\boldsymbol \lambda$.

Suppose the theorem conditions are satisfied at $\boldsymbol \lambda_0$. Let $\boldsymbol B'$ be an orthonormal set of basis vectors that span the differentiable space $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$ with the subset of vectors $\boldsymbol B$ that span the model parameter space.

Let $\tilde L_T(\boldsymbol \theta,\boldsymbol \lambda)$ be the gradient of $L_T(\cdot, \boldsymbol \lambda)$ at $\boldsymbol \theta$ with respect to the basis $\boldsymbol B$:
\begin{equation}
\tilde L_T(\boldsymbol \theta,\boldsymbol \lambda) = _{\boldsymbol B}\nabla L_T(\cdot, \boldsymbol \lambda) |_{\boldsymbol \theta}
\end{equation}

Since $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$ is the minimizer of the training loss, the gradient of $L_T(\cdot, \boldsymbol \lambda_0)$ with respect to the basis $\boldsymbol B$ must be zero at $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$:
\begin{equation}
_{\boldsymbol B}\nabla L_T(\cdot, \boldsymbol \lambda_0)|_{\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)} = \tilde L_T(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0) = 0
\end{equation}

From our assumptions, we know that there exists a neighborhood $W$ containing $\boldsymbol \lambda_0$ such that $\tilde L_T$ is continuously differentiable along directions in the differentiable space $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$. Also, the Jacobian matrix $D \tilde L_T(\cdot, \boldsymbol \lambda_0)|_{\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)}$ with respect to basis $\boldsymbol B$ is nonsingular. Therefore, by the implicit function theorem, there exist open sets $U \subseteq W$ containing $\boldsymbol \lambda_0$ and $V$ containing $\hat {\boldsymbol \theta}(\boldsymbol \lambda_0)$ and a continuously differentiable function $\gamma: U \rightarrow V$ such that for every $\boldsymbol \lambda \in U$, we have that 
\begin{equation}
\tilde L_T(\gamma(\boldsymbol \lambda), \boldsymbol \lambda) = \nabla_{B} L_T(\cdot, \boldsymbol \lambda)|_{\gamma(\boldsymbol \lambda)} = 0
\end{equation}
That is, we know that $\gamma(\boldsymbol \lambda)$ is a continuously differentiable function that minimizes $L_T(\cdot, \boldsymbol \lambda)$ in the differentiable space  $\Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)$.
Since we assumed that the differentiable space is a local optimality space of $L_T(\cdot, \boldsymbol \lambda)$ in the neighborhood $W$, then for every $\boldsymbol \lambda \in U$, 
\begin{equation}
\hat {\boldsymbol \theta}(\boldsymbol \lambda) =
\argmin_{\boldsymbol \theta} L_T(\boldsymbol \theta, \boldsymbol \lambda) =
\argmin_{\boldsymbol \theta \in \Omega^{L_T}(\hat {\boldsymbol \theta}(\boldsymbol \lambda_0), \boldsymbol \lambda_0)} L_T(\boldsymbol \theta, \boldsymbol \lambda) =
\gamma(\boldsymbol \lambda)
\end{equation}
Therefore, we have shown that if $\boldsymbol \lambda_0$ satisfies the assumptions given in the theorem, the fitted model parameters $\hat {\boldsymbol \theta}(\boldsymbol \lambda)$ is a continuously differentiable function within a neighborhood of $\boldsymbol \lambda_0$. We can then apply the chain rule to get the gradient of the validation loss.
\end{proof}

\subsection{Regression Examples}

\subsubsection{Elastic Net}\label{enet_conditions}
We show that the joint optimization problem for the Elastic Net satisfies all three conditions in Theorem~\ref{thethrm}:
\begin{itemize}
\item[] Condition 1: The elastic net solution paths are piecewise linear \citep{zou2003regression, tibshirani2013lasso}, which means that the nonzero indices of the elastic net estimates stay locally constant for almost every $\boldsymbol{\lambda}$. Therefore, $S_{\boldsymbol{\lambda}}$ is a local optimality space for $L_T(\cdot, \boldsymbol{\lambda})$. \hfill \ding{51}
\item[] Condition 2: The $\ell_1$ penalty is smooth when restricted to $S_{\boldsymbol{\lambda}}$.\hfill \ding{51}
\item[] Condition 3: The Hessian matrix of $L_T(\cdot, \boldsymbol{\lambda})$ with respect to the columns of $\boldsymbol I_{I(\boldsymbol \lambda)}$ is $\boldsymbol I_{I(\boldsymbol \lambda)}^\top \boldsymbol{X}_{T}^\top \boldsymbol{X}_{T} \boldsymbol I_{I(\boldsymbol \lambda)} + \lambda_2 \boldsymbol{I}$. This is positive definite if $\lambda_2 > 0$. \hfill \ding{51}
\end{itemize}

\subsubsection{Additive Models with Sparsity and Smoothness Penalties}
Let 
\begin{equation}
\boldsymbol{U} = \begin{bmatrix}
\boldsymbol {U}^{(i_1)} & ... & \boldsymbol {U}^{(i_{|J(\boldsymbol \lambda)|})}
\end{bmatrix}
\end{equation}
where $i_\ell \in J(\boldsymbol \lambda)$. Then
\begin{equation}
\boldsymbol{H}(\boldsymbol\lambda)
= \boldsymbol{U}^\top \boldsymbol I_T^\top \boldsymbol I_T \boldsymbol{U}
+ \lambda_0 \text{ } diag \left (
\frac{1}{||\boldsymbol {U}^{(i)}  \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)||_2} \left (
\boldsymbol I - \frac{\hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)^\top \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)}{||\boldsymbol {U}^{(i)}  \hat{\boldsymbol{\beta}}^{(i)} (\boldsymbol \lambda)||_2}
\right ) \right )
+ \epsilon \boldsymbol I
\label{eq:add_hessian}
\end{equation}
Now we check that all three conditions are satisfied.
\begin{itemize}
	\item[] Condition 1: It is difficult to formally prove that this condition is satisfied. Nonetheless, if one thinks of the dual formulation, it seems likely that the $S_{\boldsymbol{\lambda}}$ stays locally constant for small perturbations in $\boldsymbol{\lambda}$.
	\item[] Condition 2: The $\ell_1$ and $\ell_2$ penalties are smooth when restricted to $S_{\boldsymbol{\lambda}}$.\hfill \ding{51}
	\item[] Condition 3: The Hessian matrix in \eqref{eq:add_hessian} is positive definite for any $\epsilon > 0$. \hfill \ding{51}
\end{itemize}

The matrix $C(\boldsymbol \beta( \boldsymbol \lambda))$ in \eqref{eq:additive_gradient} is defined as
\begin{equation}
C(\boldsymbol \beta( \boldsymbol \lambda))
= \begin{cases}
\begin{bmatrix}
\boldsymbol{0} \\
\boldsymbol {U}^{(i)\top}  \boldsymbol{D}^{(2)\top}_{\boldsymbol{x}_i} 
sgn( \boldsymbol{D}^{(2)}_{\boldsymbol{x}_i} \boldsymbol {U}^{(i)} \hat{\boldsymbol{\beta}}^{(i)}) \\
\boldsymbol{0} \\
\end{bmatrix}
& \text{ for } i \in J(\boldsymbol \lambda) \\
\boldsymbol{0}
& \text{ for } i \not\in J(\boldsymbol \lambda) \\
\end{cases}
\end{equation}


\subsubsection{Un-pooled Sparse Group Lasso}
The Hessian in this problem is
\begin{equation}
\boldsymbol{H}(\boldsymbol\lambda) =
\frac{1}{n} \boldsymbol X_{T, I(\boldsymbol \lambda)}^\top \boldsymbol X_{T, I(\boldsymbol \lambda)}
+ diag\left(
\frac{\lambda_m}{|| \boldsymbol \theta^{(m)}||_2}
\left (
\boldsymbol I - 
\frac{\boldsymbol \theta^{(m)} \boldsymbol \theta^{(m) \top}}{|| \boldsymbol \theta^{(m)}||_2^2}
\right )
\right)
+ \epsilon \boldsymbol I
\label{eq:sgl_hessian}
\end{equation}
The logic for checking all three conditions in Theorem~\ref{thethrm} is similar to the other examples:
\begin{itemize}
	\item[] Condition 1: It is difficult to formally prove that this condition is satisfied. Nonetheless, if one thinks of the dual formulation, it seems likely that the $S_{\boldsymbol{\lambda}}$ stays locally constant for small perturbations in $\boldsymbol{\lambda}$.
	\item[] Condition 2: The $\ell_1$ and $\ell_2$ penalties are smooth when restricted to $S_{\boldsymbol{\lambda}}$.\hfill \ding{51}
	\item[] Condition 3: The Hessian matrix in \eqref{eq:sgl_hessian} is positive definite for any $\epsilon > 0$. \hfill \ding{51}
\end{itemize}


The matrix $\boldsymbol C(\hat {\boldsymbol \beta}(\boldsymbol \lambda))$ in \eqref{eq:unpooled_sgl_grad} has columns $m=1,2...,M$ 
\begin{equation}
\begin{bmatrix}
\boldsymbol 0\\
\frac{\hat {\boldsymbol \beta}^{(m)}(\boldsymbol \lambda)}{||\hat{\boldsymbol \beta}^{(m)}(\boldsymbol \lambda)||_2}\\
\boldsymbol 0\\
\end{bmatrix}
\end{equation}
where $\boldsymbol 0$ are the appropriate dimensions.

\subsubsection{Low-rank Matrix Completion}
To derive the gradient of the validation error with respect to the penalty parameters, we perform implicit differentiation of the gradient optimality conditions to get a system of linear equations. We can take the gradient of the training criterion with respect to $\eta$ and $\gamma$ to get part of our gradient optimality conditions. To get the other gradient optimality conditions, we need to manipulate the subdifferential of the objective function with respect to $\Xi$. 
Let the SVD decomposition of $\hat{\boldsymbol{\Xi}}(\boldsymbol{\lambda})$ be $\hat{\boldsymbol{U}}(\boldsymbol{\lambda}) \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})  \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top$.
When taking the gradient of the training criterion with respect to $\boldsymbol{\Xi}$, we must multiply the result by its left and right singular vectors. 
We get the following conditions:
\begin{align}
- \frac{1}{|T|} 
\hat{\boldsymbol{U}}(\boldsymbol{\lambda})^\top
\left (
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  \boldsymbol{1}^\top )^\top
- \hat{\boldsymbol{\Xi}}
\right )
+ \lambda_0 sign(\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
+ \epsilon \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda}) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
& = \boldsymbol{0}\\
- \frac{1}{|T|} 
\left (
\boldsymbol{M} 
- \boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) \boldsymbol{1}^\top 
- (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  \boldsymbol{1}^\top )^\top
- \hat{\boldsymbol{\Xi}}
\right )
\hat{\boldsymbol{V}}(\boldsymbol{\lambda})
+ \lambda_0 \hat{\boldsymbol{U}}(\boldsymbol{\lambda}) sign(\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})) 
+ \epsilon \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda}) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})
& = \boldsymbol{0}
\label{eq:grad_opt_matrix_comp}
\end{align}
Implicit differentiation of the above gradient optimality conditions combined with those for $\boldsymbol{\eta}$ and $\boldsymbol{\gamma}$ results in a system of linear equations. Solving this system gives the gradient of the validation loss with respect to $\boldsymbol{\lambda}$.


 Solving this gives us the partial derivatives of the model parameters with respect to $\boldsymbol{\lambda}$. Note that we do not solve for the partial derivative of $\hat{\boldsymbol{\Xi}}(\boldsymbol{\lambda})$ directly. Rather we solve for the partial derivatives of each of its components from the SVD decomposition:
$\frac{\partial}{\partial \boldsymbol{\lambda}}\hat{\boldsymbol{U}}(\boldsymbol{\lambda})$, $\frac{\partial}{\partial \boldsymbol{\lambda}}\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})$, and $\frac{\partial}{\partial \boldsymbol{\lambda}}\hat{\boldsymbol{V}}(\boldsymbol{\lambda})$. 



%\begin{align}
%\boldsymbol{0} & = 
%\frac{1}{|T|} 
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{U}}(\boldsymbol{\lambda})^\top
%\left (
%\boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) 
%\boldsymbol{1}^\top 
%+ \left (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  
%\boldsymbol{1}^\top \right )^\top
%+ \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\Xi}}
%\right ) \\
%& \qquad \lambda_0 sign(\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top
%+ \epsilon \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda}) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})^\top\\
%% next equation
%\boldsymbol{0} & = 
%- \frac{1}{|T|} 
%\left (
%\boldsymbol{X}_{I_r(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\eta}}(\boldsymbol{\lambda}) 
%\boldsymbol{1}^\top 
%+ \left (\boldsymbol{Z}_{I_c(\boldsymbol{\lambda})} \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\gamma}}(\boldsymbol{\lambda})  
%\boldsymbol{1}^\top \right )^\top
%+ \frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\Xi}}
%\right )
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{V}}(\boldsymbol{\lambda})
% \\
%& \qquad 
%+ \lambda_0 \hat{\boldsymbol{U}}(\boldsymbol{\lambda}) sign(\hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda})) 
%+ \epsilon \hat{\boldsymbol{\Sigma}}(\boldsymbol{\lambda}) \hat{\boldsymbol{V}}(\boldsymbol{\lambda})\\
%\end{align}
%\begin{align}
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{\Xi}}(\boldsymbol{\lambda}) & = 
%\frac{\partial}{\partial\boldsymbol{\lambda}} \hat{\boldsymbol{U}}(\boldsymbol{\lambda})
%\end{align}

\subsection{Backtracking Line Search}

Let the criterion function be $L:\mathbb{R}^n \rightarrow \mathbb{R}$. Suppose that the descent algorithm is currently at point $x$ with descent direction $\Delta x$. Backtracking line search uses a heuristic for finding a step size $t \in (0,1]$ such that the value of the criterion is minimized. The method depends on constants $\alpha  \in (0, 0.5)$ and $\beta \in (0,1)$.

\begin{algorithm}
\caption{Backtracking Line Search}
\label{alg:backtracking}
         \begin{algorithmic}
  	\STATE{Initialize $t= 1$.} \\
	\WHILE{$L(\boldsymbol x + t \boldsymbol \Delta \boldsymbol x) > L(\boldsymbol x) + \alpha t \nabla L(\boldsymbol x)^T \boldsymbol \Delta \boldsymbol x$}
	\STATE{Update $t := \beta t$}
	\ENDWHILE
	\end{algorithmic}
\end{algorithm}

\subsection{Simulation studies additional tables}

\subsubsection{Sparse add models}

\begin{table}
	\caption {\label{tab:additive} A comparison of additive models with separate smoothness penalty parameters tuned by gradient descent, Nelder-Mead, and Spearmint vs. additive models with two penalty parameters tuned by grid search. Standard errors are given in parentheses.}
	\centering
	\begin{tabular}{| l | l | l | l | l | }
		\hline
		& Num $\lambda$ & Validation Error & Test Error & \# Solves\\
		\hline
		% bayes: script_out/sparse_hc0_new_numsolves.out
		Gradient Descent & 2 & 23.87 (0.97) & 26.10 (0.86) & 13.07 \\
		\hline
		% bayes: script_out/sparse_nm0.out
		Nelder-Mead & 2 & 28.86 (1.04) & 29.97 (0.96) & 100 \\
		\hline
		% bayes: script_out/sparse_sp0.out - running on COX
		Spearmint & 2 & ? & 100 \\
		\hline
		% bayes: script_out/sparse_gs_new.out
		Grid Search & 2 & 28.71 (0.97) & 29.42 (0.96) & 100 \\
		\hline
	\end{tabular}
\end{table}

(UPDATE ME SETTINGS)
Finally, to assess how sensitive gradient descent is to its initialization points, we tried multiple starting points. For this experiment, we tried a smaller problem with 50 training, 25 validation, and 50 test observations and $p = 7$ covariates. The response was generated as the sum of $f_1$ and $f_2$ as defined above plus noise. We used the same first two initializations $(10, 1, ..., 1)$ and $(0.1, 0.01, ..., 0.01)$ and the rest were randomly selected from $10^i$ for $i = -2, ..., 1$. The results are shown in Figure \ref{fig:mult_starts}. As the number of initialization points increased, both the validation error and the test error decreased.

i need to change the line style cause confusing
\begin{figure}
	\caption{Multiple starts}
	\centering
	\includegraphics[width=0.5\textwidth]{many_inits_3_12_60_30_30_2_30_trend.png}
	\includegraphics[width=0.375\textwidth]{many_inits_3_12_60_30_30_2_30_box.png}
\end{figure}

\subsubsection{Un-pooled Sparse group lasso}
\begin{table}
\begin{tabular}{| l | l | l | l | l | l | }
	\hline
	\multicolumn{6}{|c|}{n=90, p=600, M=30}\\
	\hline
	& \# $\lambda$ & $\beta$ Error & Validation Err & Test Err & \# Solves \\
	\hline
	% bayes: script_out/sgl_hc0_1.txt
	GD & 2 & 7.36969 (0.17806) & 46.81600 (2.21461) & 49.33414 (1.35969)& 21.43333 (1.75226)\\
	\hline
	% bayes: script_out/sgl_nm0_1_90_30_p600.out
	NM & 2 & 7.31 (0.18)  & 46.37 (2.24) & 48.95 (1.35) & 100 \\
	\hline
	% running right now on bayes
	% bayes: script_out/sgl_sp0_1.out
	SP & 2 &  ? \\
	\hline
	\multicolumn{6}{|c|}{n=90, p=900, M=60}\\
	\hline
	& \# $\lambda$ & $\beta$ Error & Validation Error & Test Error & \# Solves \\
	\hline
	% bayes: script_out/sgl_hc0_2.out
	GD & 2 & 7.58 (0.21) & 45.71 (2.26) & 50.31 (1.93) & 20.77 (1.85)  \\
	\hline
	% bayes: script_out/sgl_nm0_2.out
	NM & 2 & 7.56 (0.19) & 44.95 (2.24) & 50.18 (1.82) & 100  \\
	\hline
	% bayes: script_out/sgl_sp0_2.out
	SP & 2 & 8.20 (0.20)  & 49.59 (2.27) & 56.54 (2.14) & 100 \\
	\hline
	\multicolumn{6}{|c|}{n=90, p=1200, M=100}\\
	\hline
	& \# $\lambda$ & $\beta$ Error & Validation Error & Test Error & \# Solves \\
	\hline
	% bayes: script_out/sgl_hc0_3.out
	GD & 2 & 8.27 (0.19) & 50.46 (2.30) & 57.02 (1.94) & 19.80 (1.90) \\
	\hline
	% bayes: script_out/sgl_nm0_3.out
	NM & 2 & 8.09 (0.19) & 49.92 (2.33) & 55.46 (1.89) & 100 \\
	\hline
	% bayes: script_out/sgl_sp0_3.out
	SP & 2 & 8.20 (0.20) & 49.70 (2.26) & 56.51 (2.16) & 100 \\
	\hline
\end{tabular}
\end{table}

\subsubsection{Low-rank Matrix Completion}

\begin{table}
	\caption{\label{table:matrix_completion} Matrix Completion. Standard errors are given in parentheses. \% CN $\alpha$ and \% CN $\beta$ are the percentage of correctly identified nonzero covariate groups. We abbreviated the methods as follows: Gradient Descent = GD, Nelder-Mead = NM, Spearmint = SP, Grid Search = GS}
	\centering
	\begin{tabular}{| l | l | l | l | l | l |}
		\hline
		\multicolumn{6}{|c|}{N=60, G=12, n=2}\\
		\hline
		& \# $\lambda$ & Validation Err & Test Err& $\Gamma$ Rank & Num Solves\\
		\hline
		% need to implement
		GD & 2 &  \\
		\hline
		% bayes: script_out/matrix_completion_groups_NM0_11.txt
		NM & 2 & 0.73069 (0.03751) & 0.72734 (0.03864) & 11.30000 (0.90203) & 100 \\
		\hline
		% submitted to bayes
		% bayes: script_out/matrix_completion_groups_SP0_11.txt
		SP & 2 &  & 100\\
		\hline
		\multicolumn{6}{|c|}{N=100, G=22, n=8}\\
		\hline
		& \# $\lambda$ & Validation Err & Test Err&  $\Gamma$ Rank & Num Solves\\
		\hline
		GD & 2 & ?\\
		\hline
		% bayes: script_out/matrix_completion_groups_NM0_9.txt
		NM & 2 & 0.75440 (0.04475) & 0.78089 (0.04175) & 0.80000 (0.17889) & 100\\
		\hline
		% submitted to bayes
		% bayes: script_out/matrix_completion_groups_SP0_9.txt
		SP 2 & ?\\
		\hline
	\end{tabular}
\end{table}

\bibliographystyle{agsm}
\bibliography{hillclimbing_nonsmooth_appendix}

\end{document}
